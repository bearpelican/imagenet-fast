{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation by aspect ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of center cropping, sort validation images by aspect ratio. Crop batches of these images based on the closest aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, shutil, time, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import models\n",
    "from fp16util import network_to_half, set_grad, copy_in_params\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n",
    "#print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "    parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
    "    parser.add_argument('--save-dir', type=str, default=Path.cwd(), help='Directory to save logs and models.')\n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet50')\n",
    "    # parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet18',\n",
    "    #                     choices=model_names,\n",
    "    #                     help='model architecture: ' +\n",
    "    #                     ' | '.join(model_names) +\n",
    "    #                     ' (default: resnet18)')\n",
    "    parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--epochs', default=45, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('--warmup', default=0, type=int, metavar='N',\n",
    "                        help='number of additional epochs to warmup')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 256)')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.4, type=float,\n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                        metavar='N', help='print frequency (default: 10)')\n",
    "    parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--small', action='store_true', help='start with smaller images')\n",
    "    parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                        help='evaluate model on validation set')\n",
    "    parser.add_argument('--pretrained', dest='pretrained', action='store_true', help='use pre-trained model')\n",
    "    parser.add_argument('--fp16', action='store_true', help='Run model fp16 mode.')\n",
    "    parser.add_argument('--dp', action='store_true', help='Run model fp16 mode.')\n",
    "    parser.add_argument('--sz',       default=224, type=int, help='Size of transformed image.')\n",
    "    parser.add_argument('--decay-int', default=30, type=int, help='Decay LR by 10 every decay-int epochs')\n",
    "    parser.add_argument('--loss-scale', type=float, default=1,\n",
    "                        help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "    parser.add_argument('--prof', dest='prof', action='store_true', help='Only run a few iters for profiling.')\n",
    "\n",
    "    parser.add_argument('--distributed', action='store_true', help='Run distributed training')\n",
    "    parser.add_argument('--dist-url', default='file://sync.file', type=str,\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=0, type=int,\n",
    "                        help='Used for multi-process training. Can either be manually set ' +\n",
    "                        'or automatically set by using \\'python -m multiproc\\'.')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_path = str(Path.home()/'7-8x_train_lr3d2_e68_b128_93_success/model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "args = get_parser().parse_args(['/home/paperspace/data/imagenet', '--evaluate', '--resume', resume_path])\n",
    "if args.local_rank > 0: sys.stdout = open(f'{args.save_dir}/GPU_{args.local_rank}.log', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def sort_ar(valdir):\n",
    "    if os.path.isfile('sorted_idxar.p'): return pickle.load(open('sorted_idxar.p', 'rb'))\n",
    "    val_dataset = datasets.ImageFolder(valdir)\n",
    "    sizes = [img[0].size for img in val_dataset]\n",
    "    idx_ar = [(i, round(s[0]/s[1], 5)) for i,s in enumerate(sizes)]\n",
    "    sorted_idxar = sorted(idx_ar, key=lambda x: x[1])\n",
    "    pickle.dump(sorted_idxar, open('sorted_idxar.p', 'wb'))\n",
    "    return sorted_idxar\n",
    "\n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "\n",
    "def map_idx2ar(idx_ar_sorted, batch_size):\n",
    "    ar_chunks = list(chunks(idx_ar_sorted, batch_size))\n",
    "    idx2ar = {}\n",
    "    for chunk in ar_chunks:\n",
    "        idxs, ars = list(zip(*chunk))\n",
    "        mean = round(np.mean(ars), 5)\n",
    "        for idx in idxs:\n",
    "            idx2ar[idx] = mean\n",
    "    return idx2ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None, idx_transform=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "        self.idx_transform = idx_transform\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.idx_transform(sample, index)\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "class ARSampler(Sampler):\n",
    "    def __init__(self, indices): self.indices = indices\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __iter__(self): return iter(self.indices)\n",
    "    \n",
    "\n",
    "class CropArTfm(object):\n",
    "    def __init__(self, idx2ar, target_size):\n",
    "        self.idx2ar, self.target_size = idx2ar, target_size\n",
    "    def __call__(self, img, idx):\n",
    "        target_ar = self.idx2ar[idx]\n",
    "        if target_ar < 1: \n",
    "            w = int(self.target_size/target_ar)\n",
    "            size = (w//8*8, self.target_size)\n",
    "        else: \n",
    "            h = int(self.target_size*target_ar)\n",
    "            size = (self.target_size, h//8*8)\n",
    "        return torchvision.transforms.functional.center_crop(img, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(valdir, batch_size, target_size, tensor_tfm, use_val_sampler, use_ar_sampler):\n",
    "    idx_ar_sorted = sort_ar(valdir)\n",
    "    idx_sorted, _ = zip(*idx_ar_sorted)\n",
    "    idx2ar = map_idx2ar(idx_ar_sorted, batch_size)\n",
    "    \n",
    "    if use_ar_sampler:\n",
    "        val_dataset = ValDataset(valdir, transforms.Compose(tensor_tfm), idx_transform=CropArTfm(idx2ar, target_size))\n",
    "        val_sampler = ARSampler(idx_sorted)\n",
    "        return val_dataset, val_sampler\n",
    "    \n",
    "    val_tfms = [transforms.Resize(int(args.sz*1.14)), transforms.CenterCrop(args.sz)] + tensor_tfm\n",
    "    val_dataset = datasets.ImageFolder(valdir,  transforms.Compose(val_tfms))\n",
    "    val_sampler = None\n",
    "    if use_val_sampler and args.distributed:\n",
    "        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
    "    return val_dataset, val_sampler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(traindir, valdir, bs, sz, val_bs=None, use_val_sampler=True, use_ar_sampler=False, min_scale=0.08):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    tensor_tfm = [transforms.ToTensor(), normalize]\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        traindir, transforms.Compose([\n",
    "            transforms.RandomResizedCrop(sz, scale=(min_scale, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ] + tensor_tfm))\n",
    "    train_sampler = (torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "    val_bs = val_bs or bs\n",
    "    val_dataset, val_sampler = create_validation_set(valdir, val_bs, sz, tensor_tfm, use_val_sampler, use_ar_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True, sampler=val_sampler)\n",
    "\n",
    "    return train_loader,val_loader,train_sampler,val_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item() is a recent addition, so this helps with backward compatibility.\n",
    "def to_python_float(t):\n",
    "    if hasattr(t, 'item'):\n",
    "        return t.item()\n",
    "    else:\n",
    "        return t[0]\n",
    "\n",
    "class data_prefetcher():\n",
    "    def __init__(self, loader, prefetch=True):\n",
    "        self.loader,self.prefetch = iter(loader),prefetch\n",
    "        if prefetch:\n",
    "            self.stream = torch.cuda.Stream()\n",
    "            self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def next(self):\n",
    "        if not self.prefetch:\n",
    "            input,target = next(self.loader)\n",
    "            return input.cuda(async=True),target.cuda(async=True)\n",
    "\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        input = self.next_input\n",
    "        target = self.next_target\n",
    "        self.preload()\n",
    "        return input, target\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch, start_time):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    prefetcher = data_prefetcher(val_loader)\n",
    "    input, target = prefetcher.next()\n",
    "    i = -1\n",
    "    while input is not None:\n",
    "        i += 1\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = Variable(input)\n",
    "        target_var = Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "        if args.distributed:\n",
    "            reduced_loss = reduce_tensor(loss.data)\n",
    "            prec1 = reduce_tensor(prec1)\n",
    "            prec5 = reduce_tensor(prec5)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "            \n",
    "\n",
    "        losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "        top1.update(to_python_float(prec1), input.size(0))\n",
    "        top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if args.local_rank == 0 and i % args.print_freq == 0:\n",
    "            output = ('Test: [{0}/{1}]\\t' \\\n",
    "                    + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                    + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                    + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n",
    "                    + 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})').format(\n",
    "                    i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    top1=top1, top5=top5)\n",
    "            print(output)\n",
    "            with open(f'{args.save_dir}/full.log', 'a') as f:\n",
    "                f.write(output + '\\n')\n",
    "\n",
    "        input, target = prefetcher.next()\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    print(f'~~{epoch}\\t{float(time_diff.total_seconds() / 3600.0)}\\t{top5.avg:.3f}\\n')\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, f'{args.save_dir}/model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    size = dist.get_world_size()\n",
    "    # rt /= args.world_size\n",
    "    rt /= size\n",
    "    return rt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.fp16 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~epoch\thours\ttop1Accuracy\n",
      "\n",
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "print(\"~~epoch\\thours\\ttop1Accuracy\\n\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "if args.distributed:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url)\n",
    "    print('Distributed: init_process_group success')\n",
    "\n",
    "if args.fp16: assert torch.backends.cudnn.enabled, \"fp16 mode requires cudnn backend to be enabled.\"\n",
    "\n",
    "# create model\n",
    "#     if args.pretrained: model = models.__dict__[args.arch](pretrained=True)\n",
    "#     else: model = models.__dict__[args.arch]()\n",
    "# AS: force use resnet50 for now, until we figure out whether to upload model directory\n",
    "import resnet\n",
    "model = resnet.resnet50()\n",
    "\n",
    "print(\"Loaded model\")\n",
    "\n",
    "model = model.cuda()\n",
    "n_dev = torch.cuda.device_count()\n",
    "if args.fp16: model = network_to_half(model)\n",
    "if args.distributed: model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "elif args.dp:\n",
    "    model = nn.DataParallel(model)\n",
    "    args.batch_size *= n_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined loss and optimizer\n"
     ]
    }
   ],
   "source": [
    "global param_copy\n",
    "if args.fp16:\n",
    "    param_copy = [param.clone().type(torch.cuda.FloatTensor).detach() for param in model.parameters()]\n",
    "    for param in param_copy: param.requires_grad = True\n",
    "else: param_copy = list(model.parameters())\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(param_copy, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "print(\"Defined loss and optimizer\")\n",
    "\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWrap(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    def forward(self, x):\n",
    "        return self.module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to wrap inside module - sinde checkpoint had a Distributed wrapper around it\n",
    "model = DWrap(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally resume from a checkpoint\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(0))\n",
    "#             checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(args.gpu))\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    else: print(\"=> no checkpoint found at '{}'\".format(args.resume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_wrapper_idx(fn):\n",
    "    return lambda x,idx: (fn(x),idx)\n",
    "def tfm_wrapper(fn):\n",
    "    return lambda x,idx: fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCropArTfm(object):\n",
    "    def __init__(self, idx2ar, target_size):\n",
    "        self.idx2ar, self.target_size = idx2ar, target_size\n",
    "        self.rc = transforms.RandomCrop(0)\n",
    "    def __call__(self, img, idx):\n",
    "        target_ar = self.idx2ar[idx]\n",
    "        if target_ar < 1: \n",
    "            w = int(self.target_size/target_ar)\n",
    "            size = (w//8*8, self.target_size)\n",
    "        else: \n",
    "            h = int(self.target_size*target_ar)\n",
    "            size = (self.target_size, h//8*8)\n",
    "        self.rc.size = size\n",
    "        print(size)\n",
    "        return self.rc(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            for tfm in self.transform:\n",
    "                if isinstance(tfm, CropArTfm) or isinstance(tfm, RandomCropArTfm): sample = tfm(sample, index)\n",
    "                else: sample = tfm(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdir = os.path.join(args.data, 'validation')\n",
    "val_bs = 128\n",
    "target_size = 288\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "tensor_tfm = [transforms.ToTensor(), normalize]\n",
    "\n",
    "idx_ar_sorted = sort_ar(valdir)\n",
    "idx_sorted, _ = zip(*idx_ar_sorted)\n",
    "idx2ar = map_idx2ar(idx_ar_sorted, val_bs)\n",
    "\n",
    "val_dataset_ar = ValDataset(valdir, [CropArTfm(idx2ar, target_size)] + tensor_tfm)\n",
    "val_sampler_ar = ARSampler(idx_sorted)\n",
    "\n",
    "val_ar_tfms = [transforms.Resize(int(target_size*1.14)), CropArTfm(idx2ar, target_size)]\n",
    "val_dataset_ar_rs = ValDataset(valdir, val_ar_tfms+tensor_tfm)\n",
    "\n",
    "val_tfms = [transforms.Resize(int(target_size*1.14)), transforms.CenterCrop(target_size)] + tensor_tfm\n",
    "val_dataset = datasets.ImageFolder(valdir,  transforms.Compose(val_tfms))\n",
    "# val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test validation with aspect ratio transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/391]\tTime 7.970 (7.970)\tLoss 1.5127 (1.5127)\tPrec@1 66.406 (66.406)\tPrec@5 87.500 (87.500)\n",
      "Test: [10/391]\tTime 0.500 (2.160)\tLoss 1.0293 (1.0240)\tPrec@1 79.688 (74.077)\tPrec@5 92.969 (92.543)\n",
      "Test: [20/391]\tTime 3.342 (1.505)\tLoss 1.0449 (1.0817)\tPrec@1 71.094 (72.693)\tPrec@5 92.969 (92.225)\n",
      "Test: [30/391]\tTime 0.445 (1.530)\tLoss 0.8716 (1.0731)\tPrec@1 80.469 (73.236)\tPrec@5 92.969 (91.935)\n",
      "Test: [40/391]\tTime 0.441 (1.265)\tLoss 1.3506 (1.0415)\tPrec@1 64.062 (73.990)\tPrec@5 89.062 (92.226)\n",
      "Test: [50/391]\tTime 0.441 (1.103)\tLoss 1.2803 (1.0966)\tPrec@1 72.656 (73.131)\tPrec@5 88.281 (91.376)\n",
      "Test: [60/391]\tTime 0.442 (0.995)\tLoss 1.6611 (1.1348)\tPrec@1 57.031 (72.080)\tPrec@5 85.156 (90.868)\n",
      "Test: [70/391]\tTime 0.442 (0.917)\tLoss 1.1611 (1.1243)\tPrec@1 72.656 (72.359)\tPrec@5 91.406 (91.032)\n",
      "Test: [80/391]\tTime 3.170 (0.947)\tLoss 1.0908 (1.1000)\tPrec@1 72.656 (72.830)\tPrec@5 92.188 (91.310)\n",
      "Test: [90/391]\tTime 0.395 (0.956)\tLoss 1.3398 (1.0952)\tPrec@1 67.188 (72.888)\tPrec@5 90.625 (91.389)\n",
      "Test: [100/391]\tTime 2.879 (1.009)\tLoss 1.1787 (1.0931)\tPrec@1 70.312 (72.896)\tPrec@5 90.625 (91.422)\n",
      "Test: [110/391]\tTime 0.339 (0.948)\tLoss 1.7539 (1.0995)\tPrec@1 58.594 (72.762)\tPrec@5 82.031 (91.378)\n",
      "Test: [120/391]\tTime 0.338 (0.898)\tLoss 1.5889 (1.1419)\tPrec@1 64.844 (71.849)\tPrec@5 84.375 (90.786)\n",
      "Test: [130/391]\tTime 2.820 (0.910)\tLoss 1.2480 (1.1397)\tPrec@1 65.625 (71.935)\tPrec@5 86.719 (90.714)\n",
      "Test: [140/391]\tTime 0.393 (0.905)\tLoss 0.9453 (1.1265)\tPrec@1 77.344 (72.219)\tPrec@5 90.625 (90.841)\n",
      "Test: [150/391]\tTime 0.416 (0.913)\tLoss 0.6436 (1.1166)\tPrec@1 83.594 (72.449)\tPrec@5 94.531 (90.889)\n",
      "Test: [160/391]\tTime 0.425 (0.900)\tLoss 1.1221 (1.0994)\tPrec@1 68.750 (72.884)\tPrec@5 92.969 (91.081)\n",
      "Test: [170/391]\tTime 0.440 (0.884)\tLoss 0.8008 (1.0903)\tPrec@1 78.125 (73.104)\tPrec@5 94.531 (91.178)\n",
      "Test: [180/391]\tTime 0.453 (0.875)\tLoss 0.4363 (1.0811)\tPrec@1 89.062 (73.261)\tPrec@5 99.219 (91.264)\n",
      "Test: [190/391]\tTime 0.453 (0.853)\tLoss 0.6284 (1.0700)\tPrec@1 85.156 (73.462)\tPrec@5 93.750 (91.455)\n",
      "Test: [200/391]\tTime 0.453 (0.833)\tLoss 1.2881 (1.0531)\tPrec@1 66.406 (73.850)\tPrec@5 91.406 (91.647)\n",
      "Test: [210/391]\tTime 0.452 (0.815)\tLoss 0.8555 (1.0432)\tPrec@1 78.125 (74.071)\tPrec@5 95.312 (91.825)\n",
      "Test: [220/391]\tTime 0.453 (0.799)\tLoss 0.5269 (1.0338)\tPrec@1 89.062 (74.208)\tPrec@5 95.312 (91.968)\n",
      "Test: [230/391]\tTime 0.454 (0.784)\tLoss 0.5527 (1.0266)\tPrec@1 87.500 (74.418)\tPrec@5 96.094 (92.045)\n",
      "Test: [240/391]\tTime 0.455 (0.770)\tLoss 1.4688 (1.0385)\tPrec@1 60.156 (74.157)\tPrec@5 89.062 (91.915)\n",
      "Test: [250/391]\tTime 0.454 (0.758)\tLoss 1.0068 (1.0464)\tPrec@1 73.438 (73.976)\tPrec@5 90.625 (91.805)\n",
      "Test: [260/391]\tTime 0.454 (0.746)\tLoss 1.6133 (1.0527)\tPrec@1 63.281 (73.851)\tPrec@5 84.375 (91.691)\n",
      "Test: [270/391]\tTime 0.454 (0.735)\tLoss 1.0322 (1.0572)\tPrec@1 74.219 (73.755)\tPrec@5 90.625 (91.634)\n",
      "Test: [280/391]\tTime 0.454 (0.725)\tLoss 1.1885 (1.0607)\tPrec@1 69.531 (73.685)\tPrec@5 88.281 (91.545)\n",
      "Test: [290/391]\tTime 0.454 (0.716)\tLoss 1.5293 (1.0675)\tPrec@1 57.812 (73.507)\tPrec@5 87.500 (91.465)\n",
      "Test: [300/391]\tTime 0.462 (0.707)\tLoss 0.9409 (1.0700)\tPrec@1 74.219 (73.396)\tPrec@5 91.406 (91.440)\n",
      "Test: [310/391]\tTime 0.454 (0.699)\tLoss 1.6201 (1.0661)\tPrec@1 55.469 (73.463)\tPrec@5 85.156 (91.489)\n",
      "Test: [320/391]\tTime 3.453 (0.707)\tLoss 1.0801 (1.0640)\tPrec@1 71.875 (73.491)\tPrec@5 92.188 (91.513)\n",
      "Test: [330/391]\tTime 0.509 (0.716)\tLoss 0.9121 (1.0598)\tPrec@1 74.219 (73.598)\tPrec@5 96.094 (91.588)\n",
      "Test: [340/391]\tTime 0.519 (0.725)\tLoss 0.8989 (1.0569)\tPrec@1 80.469 (73.664)\tPrec@5 92.969 (91.624)\n",
      "Test: [350/391]\tTime 0.613 (0.719)\tLoss 0.6880 (1.0522)\tPrec@1 85.156 (73.758)\tPrec@5 95.312 (91.702)\n",
      "Test: [360/391]\tTime 0.519 (0.720)\tLoss 0.6689 (1.0473)\tPrec@1 85.938 (73.853)\tPrec@5 95.312 (91.794)\n",
      "Test: [370/391]\tTime 0.523 (0.715)\tLoss 1.0957 (1.0443)\tPrec@1 75.781 (73.960)\tPrec@5 91.406 (91.846)\n",
      "Test: [380/391]\tTime 0.674 (0.710)\tLoss 1.0078 (1.0430)\tPrec@1 71.094 (73.999)\tPrec@5 92.969 (91.870)\n",
      "Test: [390/391]\tTime 6.181 (0.792)\tLoss 1.5479 (1.0473)\tPrec@1 76.250 (73.944)\tPrec@5 90.000 (91.842)\n",
      "~~0\t0.3455530580555556\t91.842\n",
      "\n",
      " * Prec@1 73.944 Prec@5 91.842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.944"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "validate(val_loader, model, criterion, 0, start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test original validation transforms (Sorted by aspect ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/391]\tTime 1.912 (1.912)\tLoss 1.3594 (1.3594)\tPrec@1 67.188 (67.188)\tPrec@5 86.719 (86.719)\n",
      "Test: [10/391]\tTime 0.088 (0.400)\tLoss 1.0459 (0.9636)\tPrec@1 79.688 (75.000)\tPrec@5 89.062 (92.259)\n",
      "Test: [20/391]\tTime 0.088 (0.392)\tLoss 0.9121 (1.0274)\tPrec@1 75.000 (73.772)\tPrec@5 95.312 (91.592)\n",
      "Test: [30/391]\tTime 0.088 (0.350)\tLoss 0.8262 (1.0025)\tPrec@1 82.031 (74.320)\tPrec@5 93.750 (92.087)\n",
      "Test: [40/391]\tTime 0.088 (0.357)\tLoss 1.0703 (0.9653)\tPrec@1 71.094 (75.305)\tPrec@5 92.969 (92.530)\n",
      "Test: [50/391]\tTime 0.090 (0.337)\tLoss 1.2402 (1.0169)\tPrec@1 69.531 (74.357)\tPrec@5 92.969 (91.881)\n",
      "Test: [60/391]\tTime 0.088 (0.340)\tLoss 1.7568 (1.0623)\tPrec@1 54.688 (73.335)\tPrec@5 83.594 (91.304)\n",
      "Test: [70/391]\tTime 0.088 (0.331)\tLoss 1.1191 (1.0536)\tPrec@1 74.219 (73.537)\tPrec@5 89.844 (91.384)\n",
      "Test: [80/391]\tTime 0.088 (0.331)\tLoss 0.9688 (1.0258)\tPrec@1 75.000 (74.199)\tPrec@5 90.625 (91.763)\n",
      "Test: [90/391]\tTime 0.088 (0.324)\tLoss 1.0059 (1.0122)\tPrec@1 73.438 (74.511)\tPrec@5 93.750 (92.033)\n",
      "Test: [100/391]\tTime 0.088 (0.328)\tLoss 0.8213 (0.9909)\tPrec@1 78.125 (74.954)\tPrec@5 94.531 (92.358)\n",
      "Test: [110/391]\tTime 0.094 (0.322)\tLoss 1.1328 (0.9812)\tPrec@1 71.875 (75.197)\tPrec@5 89.844 (92.483)\n",
      "Test: [120/391]\tTime 0.088 (0.325)\tLoss 1.2432 (0.9991)\tPrec@1 71.875 (74.858)\tPrec@5 89.062 (92.246)\n",
      "Test: [130/391]\tTime 0.100 (0.321)\tLoss 0.8037 (0.9871)\tPrec@1 78.906 (75.066)\tPrec@5 92.188 (92.343)\n",
      "Test: [140/391]\tTime 0.088 (0.325)\tLoss 0.8145 (0.9729)\tPrec@1 79.688 (75.366)\tPrec@5 95.312 (92.537)\n",
      "Test: [150/391]\tTime 0.088 (0.321)\tLoss 0.5723 (0.9639)\tPrec@1 84.375 (75.590)\tPrec@5 96.094 (92.627)\n",
      "Test: [160/391]\tTime 0.088 (0.322)\tLoss 1.0430 (0.9493)\tPrec@1 72.656 (76.009)\tPrec@5 92.188 (92.770)\n",
      "Test: [170/391]\tTime 0.088 (0.318)\tLoss 0.5981 (0.9418)\tPrec@1 81.250 (76.183)\tPrec@5 96.875 (92.845)\n",
      "Test: [180/391]\tTime 0.088 (0.322)\tLoss 0.4099 (0.9340)\tPrec@1 87.500 (76.308)\tPrec@5 99.219 (92.908)\n",
      "Test: [190/391]\tTime 0.101 (0.318)\tLoss 0.5298 (0.9302)\tPrec@1 87.500 (76.383)\tPrec@5 95.312 (92.993)\n",
      "Test: [200/391]\tTime 0.088 (0.319)\tLoss 1.1523 (0.9175)\tPrec@1 64.844 (76.636)\tPrec@5 95.312 (93.151)\n",
      "Test: [210/391]\tTime 0.088 (0.316)\tLoss 0.8389 (0.9106)\tPrec@1 71.875 (76.759)\tPrec@5 93.750 (93.280)\n",
      "Test: [220/391]\tTime 0.088 (0.318)\tLoss 0.4783 (0.9036)\tPrec@1 87.500 (76.796)\tPrec@5 96.094 (93.425)\n",
      "Test: [230/391]\tTime 0.088 (0.316)\tLoss 0.5186 (0.8972)\tPrec@1 89.062 (76.972)\tPrec@5 97.656 (93.493)\n",
      "Test: [240/391]\tTime 0.130 (0.318)\tLoss 1.3730 (0.9086)\tPrec@1 61.719 (76.699)\tPrec@5 88.281 (93.355)\n",
      "Test: [250/391]\tTime 0.311 (0.316)\tLoss 0.9868 (0.9176)\tPrec@1 75.000 (76.528)\tPrec@5 89.844 (93.258)\n",
      "Test: [260/391]\tTime 0.288 (0.317)\tLoss 1.3574 (0.9259)\tPrec@1 66.406 (76.404)\tPrec@5 85.938 (93.130)\n",
      "Test: [270/391]\tTime 0.537 (0.317)\tLoss 1.1650 (0.9322)\tPrec@1 72.656 (76.283)\tPrec@5 89.844 (93.067)\n",
      "Test: [280/391]\tTime 0.787 (0.318)\tLoss 1.1777 (0.9386)\tPrec@1 74.219 (76.184)\tPrec@5 87.500 (92.927)\n",
      "Test: [290/391]\tTime 0.151 (0.317)\tLoss 1.3057 (0.9470)\tPrec@1 63.281 (76.007)\tPrec@5 89.062 (92.832)\n",
      "Test: [300/391]\tTime 0.594 (0.317)\tLoss 0.9692 (0.9517)\tPrec@1 71.094 (75.836)\tPrec@5 93.750 (92.774)\n",
      "Test: [310/391]\tTime 0.231 (0.317)\tLoss 1.5723 (0.9498)\tPrec@1 54.688 (75.867)\tPrec@5 85.938 (92.800)\n",
      "Test: [320/391]\tTime 0.478 (0.318)\tLoss 0.9346 (0.9483)\tPrec@1 73.438 (75.862)\tPrec@5 93.750 (92.840)\n",
      "Test: [330/391]\tTime 0.088 (0.317)\tLoss 0.8560 (0.9449)\tPrec@1 76.562 (75.946)\tPrec@5 96.094 (92.886)\n",
      "Test: [340/391]\tTime 0.852 (0.317)\tLoss 0.9360 (0.9434)\tPrec@1 79.688 (75.999)\tPrec@5 93.750 (92.916)\n",
      "Test: [350/391]\tTime 0.088 (0.317)\tLoss 0.6367 (0.9399)\tPrec@1 85.156 (76.053)\tPrec@5 96.875 (92.953)\n",
      "Test: [360/391]\tTime 0.716 (0.317)\tLoss 0.4917 (0.9344)\tPrec@1 89.844 (76.153)\tPrec@5 96.094 (93.034)\n",
      "Test: [370/391]\tTime 0.088 (0.316)\tLoss 1.0430 (0.9325)\tPrec@1 78.125 (76.205)\tPrec@5 92.188 (93.080)\n",
      "Test: [380/391]\tTime 0.459 (0.316)\tLoss 0.7217 (0.9330)\tPrec@1 78.906 (76.191)\tPrec@5 95.312 (93.075)\n",
      "Test: [390/391]\tTime 1.904 (0.319)\tLoss 1.2861 (0.9355)\tPrec@1 71.250 (76.144)\tPrec@5 86.250 (93.046)\n",
      "~~0\t0.31047149611111113\t93.046\n",
      "\n",
      " * Prec@1 76.144 Prec@5 93.046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.144"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "validate(val_loader, model, criterion, 0, start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AR with size*1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/391]\tTime 2.462 (2.462)\tLoss 1.0381 (1.0381)\tPrec@1 73.438 (73.438)\tPrec@5 91.406 (91.406)\n",
      "Test: [10/391]\tTime 0.138 (0.494)\tLoss 0.9663 (0.8747)\tPrec@1 79.688 (77.202)\tPrec@5 92.188 (94.247)\n",
      "Test: [20/391]\tTime 0.123 (0.463)\tLoss 0.9185 (0.9419)\tPrec@1 78.125 (75.930)\tPrec@5 94.531 (93.824)\n",
      "Test: [30/391]\tTime 0.121 (0.408)\tLoss 0.7988 (0.9348)\tPrec@1 85.938 (76.260)\tPrec@5 92.969 (93.800)\n",
      "Test: [40/391]\tTime 0.133 (0.411)\tLoss 1.0264 (0.9115)\tPrec@1 73.438 (77.115)\tPrec@5 93.750 (94.074)\n",
      "Test: [50/391]\tTime 0.113 (0.387)\tLoss 1.1367 (0.9567)\tPrec@1 73.438 (76.149)\tPrec@5 91.406 (93.367)\n",
      "Test: [60/391]\tTime 0.113 (0.386)\tLoss 1.6260 (0.9970)\tPrec@1 56.250 (75.128)\tPrec@5 85.938 (92.841)\n",
      "Test: [70/391]\tTime 0.113 (0.373)\tLoss 1.0781 (0.9921)\tPrec@1 73.438 (75.253)\tPrec@5 92.969 (92.848)\n",
      "Test: [80/391]\tTime 0.105 (0.371)\tLoss 0.8721 (0.9677)\tPrec@1 76.562 (75.791)\tPrec@5 93.750 (93.142)\n",
      "Test: [90/391]\tTime 0.109 (0.361)\tLoss 0.8960 (0.9565)\tPrec@1 78.125 (75.953)\tPrec@5 96.094 (93.286)\n",
      "Test: [100/391]\tTime 0.088 (0.362)\tLoss 0.8213 (0.9394)\tPrec@1 78.125 (76.222)\tPrec@5 94.531 (93.487)\n",
      "Test: [110/391]\tTime 0.088 (0.353)\tLoss 1.1328 (0.9344)\tPrec@1 71.875 (76.351)\tPrec@5 89.844 (93.511)\n",
      "Test: [120/391]\tTime 0.088 (0.354)\tLoss 1.2432 (0.9561)\tPrec@1 71.875 (75.917)\tPrec@5 89.062 (93.188)\n",
      "Test: [130/391]\tTime 0.095 (0.348)\tLoss 0.8730 (0.9478)\tPrec@1 74.219 (75.984)\tPrec@5 92.188 (93.213)\n",
      "Test: [140/391]\tTime 0.100 (0.349)\tLoss 0.8003 (0.9357)\tPrec@1 79.688 (76.180)\tPrec@5 93.750 (93.351)\n",
      "Test: [150/391]\tTime 0.104 (0.345)\tLoss 0.5625 (0.9280)\tPrec@1 85.156 (76.371)\tPrec@5 97.656 (93.419)\n",
      "Test: [160/391]\tTime 0.109 (0.346)\tLoss 1.0293 (0.9153)\tPrec@1 74.219 (76.747)\tPrec@5 92.188 (93.527)\n",
      "Test: [170/391]\tTime 0.110 (0.342)\tLoss 0.6084 (0.9092)\tPrec@1 84.375 (76.919)\tPrec@5 96.875 (93.563)\n",
      "Test: [180/391]\tTime 0.112 (0.346)\tLoss 0.3875 (0.9023)\tPrec@1 90.625 (77.046)\tPrec@5 98.438 (93.590)\n",
      "Test: [190/391]\tTime 0.117 (0.343)\tLoss 0.6128 (0.9005)\tPrec@1 85.156 (77.049)\tPrec@5 96.094 (93.648)\n",
      "Test: [200/391]\tTime 0.112 (0.346)\tLoss 1.1836 (0.8907)\tPrec@1 65.625 (77.282)\tPrec@5 92.969 (93.754)\n",
      "Test: [210/391]\tTime 0.112 (0.343)\tLoss 0.8467 (0.8864)\tPrec@1 76.562 (77.377)\tPrec@5 96.094 (93.846)\n",
      "Test: [220/391]\tTime 0.112 (0.347)\tLoss 0.5015 (0.8814)\tPrec@1 88.281 (77.418)\tPrec@5 95.312 (93.941)\n",
      "Test: [230/391]\tTime 0.112 (0.344)\tLoss 0.5239 (0.8767)\tPrec@1 89.844 (77.547)\tPrec@5 97.656 (93.990)\n",
      "Test: [240/391]\tTime 0.112 (0.347)\tLoss 1.3008 (0.8858)\tPrec@1 63.281 (77.321)\tPrec@5 91.406 (93.915)\n",
      "Test: [250/391]\tTime 0.112 (0.345)\tLoss 0.9199 (0.8933)\tPrec@1 75.781 (77.151)\tPrec@5 92.969 (93.834)\n",
      "Test: [260/391]\tTime 0.112 (0.346)\tLoss 1.3320 (0.9000)\tPrec@1 66.406 (77.056)\tPrec@5 85.938 (93.717)\n",
      "Test: [270/391]\tTime 0.111 (0.345)\tLoss 1.0078 (0.9051)\tPrec@1 75.000 (76.932)\tPrec@5 90.625 (93.664)\n",
      "Test: [280/391]\tTime 0.112 (0.346)\tLoss 1.1553 (0.9104)\tPrec@1 76.562 (76.846)\tPrec@5 89.062 (93.558)\n",
      "Test: [290/391]\tTime 0.112 (0.343)\tLoss 1.3145 (0.9187)\tPrec@1 64.844 (76.646)\tPrec@5 89.844 (93.422)\n",
      "Test: [300/391]\tTime 0.112 (0.344)\tLoss 0.9146 (0.9228)\tPrec@1 71.875 (76.482)\tPrec@5 93.750 (93.366)\n",
      "Test: [310/391]\tTime 0.116 (0.342)\tLoss 1.4590 (0.9210)\tPrec@1 57.812 (76.532)\tPrec@5 86.719 (93.381)\n",
      "Test: [320/391]\tTime 0.118 (0.343)\tLoss 0.9023 (0.9198)\tPrec@1 77.344 (76.536)\tPrec@5 92.188 (93.404)\n",
      "Test: [330/391]\tTime 0.121 (0.342)\tLoss 0.7964 (0.9171)\tPrec@1 78.906 (76.619)\tPrec@5 96.094 (93.460)\n",
      "Test: [340/391]\tTime 0.128 (0.342)\tLoss 0.9033 (0.9155)\tPrec@1 80.469 (76.677)\tPrec@5 93.750 (93.484)\n",
      "Test: [350/391]\tTime 0.128 (0.341)\tLoss 0.6709 (0.9128)\tPrec@1 82.031 (76.763)\tPrec@5 96.094 (93.532)\n",
      "Test: [360/391]\tTime 0.129 (0.342)\tLoss 0.5132 (0.9091)\tPrec@1 89.844 (76.850)\tPrec@5 96.094 (93.596)\n",
      "Test: [370/391]\tTime 0.129 (0.341)\tLoss 1.0059 (0.9075)\tPrec@1 79.688 (76.908)\tPrec@5 92.969 (93.653)\n",
      "Test: [380/391]\tTime 0.129 (0.341)\tLoss 0.7344 (0.9076)\tPrec@1 79.688 (76.934)\tPrec@5 95.312 (93.647)\n",
      "Test: [390/391]\tTime 0.176 (0.342)\tLoss 0.9429 (0.9083)\tPrec@1 77.500 (76.894)\tPrec@5 92.500 (93.668)\n",
      "~~0\t0.3737668822222222\t93.668\n",
      "\n",
      " * Prec@1 76.894 Prec@5 93.668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.894"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "validate(val_loader, model, criterion, 0, start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_scale = 0.5\n",
    "trn_tfms = [\n",
    "        transforms.RandomResizedCrop(args.sz, scale=(min_scale, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tensor_tfm\n",
    "aug_dataset = datasets.ImageFolder(valdir, transforms.Compose(trn_tfms))\n",
    "\n",
    "aug_loader = torch.utils.data.DataLoader(\n",
    "    aug_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "def tta(val_loader, aug_loader, model, criterion, epoch, start_time):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    val_iter = iter(val_loader)\n",
    "    aug_iters = [iter(aug_loader),iter(aug_loader),iter(aug_loader),iter(aug_loader)]\n",
    "    for i in range(len(val_loader)):\n",
    "        def get_output(dl_iter):\n",
    "            input,target = next(dl_iter)\n",
    "            target = target.cuda(async=True)\n",
    "            input = input.cuda(async=True)\n",
    "            input_var = Variable(input)\n",
    "            target_var = Variable(target)\n",
    "\n",
    "            # compute output\n",
    "            with torch.no_grad():\n",
    "                output = model(input_var)\n",
    "                loss = criterion(output, target_var)\n",
    "            return output, loss, input, target\n",
    "        \n",
    "        output,loss,input,target = get_output(val_iter)\n",
    "        for aug_iter in aug_iters:\n",
    "            o,l,_,_ = get_output(aug_iter)\n",
    "            output.add_(o)\n",
    "            loss.add_(l)\n",
    "        loss.div_(5)\n",
    "        \n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "        if args.distributed:\n",
    "            reduced_loss = reduce_tensor(loss.data)\n",
    "            prec1 = reduce_tensor(prec1)\n",
    "            prec5 = reduce_tensor(prec5)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "            \n",
    "\n",
    "        losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "        top1.update(to_python_float(prec1), input.size(0))\n",
    "        top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if args.local_rank == 0 and i % args.print_freq == 0:\n",
    "            output = ('Test: [{0}/{1}]\\t' \\\n",
    "                    + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                    + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                    + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n",
    "                    + 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})').format(\n",
    "                    i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    top1=top1, top5=top5)\n",
    "            print(output)\n",
    "            with open(f'{args.save_dir}/full.log', 'a') as f:\n",
    "                f.write(output + '\\n')\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    print(f'~~{epoch}\\t{float(time_diff.total_seconds() / 3600.0)}\\t{top5.avg:.3f}\\n')\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/391]\tTime 10.918 (10.918)\tLoss 1.2510 (1.2510)\tPrec@1 71.094 (71.094)\tPrec@5 91.406 (91.406)\n",
      "Test: [10/391]\tTime 1.190 (2.074)\tLoss 1.1035 (1.0194)\tPrec@1 78.906 (76.847)\tPrec@5 94.531 (93.750)\n",
      "Test: [20/391]\tTime 1.186 (1.651)\tLoss 1.1348 (1.1066)\tPrec@1 78.906 (75.298)\tPrec@5 94.531 (92.857)\n",
      "Test: [30/391]\tTime 1.188 (1.503)\tLoss 0.9624 (1.0940)\tPrec@1 84.375 (75.554)\tPrec@5 93.750 (93.070)\n",
      "Test: [40/391]\tTime 1.197 (1.428)\tLoss 1.2168 (1.0660)\tPrec@1 71.875 (76.010)\tPrec@5 89.844 (93.388)\n",
      "Test: [50/391]\tTime 1.195 (1.382)\tLoss 1.3574 (1.1250)\tPrec@1 71.094 (75.107)\tPrec@5 91.406 (92.708)\n",
      "Test: [60/391]\tTime 1.197 (1.352)\tLoss 2.0273 (1.1816)\tPrec@1 58.594 (74.103)\tPrec@5 84.375 (92.098)\n",
      "Test: [70/391]\tTime 1.203 (1.331)\tLoss 1.3018 (1.1768)\tPrec@1 72.656 (74.252)\tPrec@5 91.406 (92.088)\n",
      "Test: [80/391]\tTime 1.217 (1.315)\tLoss 1.0947 (1.1490)\tPrec@1 75.000 (74.682)\tPrec@5 92.969 (92.380)\n",
      "Test: [90/391]\tTime 1.207 (1.303)\tLoss 1.1387 (1.1390)\tPrec@1 75.781 (74.966)\tPrec@5 91.406 (92.488)\n",
      "Test: [100/391]\tTime 1.260 (1.296)\tLoss 0.9790 (1.1175)\tPrec@1 77.344 (75.255)\tPrec@5 94.531 (92.744)\n",
      "Test: [110/391]\tTime 1.247 (1.292)\tLoss 1.3174 (1.1092)\tPrec@1 71.094 (75.429)\tPrec@5 88.281 (92.736)\n",
      "Test: [120/391]\tTime 1.249 (1.288)\tLoss 1.4160 (1.1332)\tPrec@1 74.219 (75.006)\tPrec@5 87.500 (92.368)\n",
      "Test: [130/391]\tTime 1.252 (1.285)\tLoss 0.9634 (1.1206)\tPrec@1 76.562 (75.131)\tPrec@5 92.188 (92.444)\n",
      "Test: [140/391]\tTime 1.250 (1.283)\tLoss 1.0098 (1.1054)\tPrec@1 78.125 (75.360)\tPrec@5 91.406 (92.570)\n",
      "Test: [150/391]\tTime 1.253 (1.281)\tLoss 0.6260 (1.0962)\tPrec@1 85.938 (75.579)\tPrec@5 96.094 (92.596)\n",
      "Test: [160/391]\tTime 1.253 (1.279)\tLoss 1.2393 (1.0808)\tPrec@1 69.531 (75.932)\tPrec@5 92.969 (92.712)\n",
      "Test: [170/391]\tTime 1.258 (1.277)\tLoss 0.6748 (1.0731)\tPrec@1 82.812 (76.110)\tPrec@5 96.094 (92.763)\n",
      "Test: [180/391]\tTime 1.252 (1.276)\tLoss 0.4680 (1.0645)\tPrec@1 88.281 (76.234)\tPrec@5 97.656 (92.822)\n",
      "Test: [190/391]\tTime 1.251 (1.275)\tLoss 0.6328 (1.0644)\tPrec@1 85.938 (76.252)\tPrec@5 95.312 (92.846)\n",
      "Test: [200/391]\tTime 1.252 (1.274)\tLoss 1.2998 (1.0520)\tPrec@1 68.750 (76.493)\tPrec@5 96.094 (92.980)\n",
      "Test: [210/391]\tTime 1.276 (1.273)\tLoss 1.0605 (1.0458)\tPrec@1 73.438 (76.596)\tPrec@5 93.750 (93.080)\n",
      "Test: [220/391]\tTime 1.255 (1.272)\tLoss 0.6094 (1.0392)\tPrec@1 88.281 (76.637)\tPrec@5 95.312 (93.220)\n",
      "Test: [230/391]\tTime 1.253 (1.271)\tLoss 0.6240 (1.0325)\tPrec@1 85.156 (76.847)\tPrec@5 98.438 (93.283)\n",
      "Test: [240/391]\tTime 1.251 (1.270)\tLoss 1.4199 (1.0441)\tPrec@1 62.500 (76.605)\tPrec@5 90.625 (93.160)\n",
      "Test: [250/391]\tTime 1.260 (1.270)\tLoss 1.1406 (1.0531)\tPrec@1 75.781 (76.444)\tPrec@5 90.625 (93.071)\n",
      "Test: [260/391]\tTime 1.253 (1.269)\tLoss 1.5908 (1.0619)\tPrec@1 63.281 (76.323)\tPrec@5 84.375 (92.951)\n",
      "Test: [270/391]\tTime 1.253 (1.269)\tLoss 1.2510 (1.0684)\tPrec@1 73.438 (76.193)\tPrec@5 91.406 (92.931)\n",
      "Test: [280/391]\tTime 1.261 (1.268)\tLoss 1.5732 (1.0757)\tPrec@1 70.312 (76.068)\tPrec@5 83.594 (92.782)\n",
      "Test: [290/391]\tTime 1.259 (1.268)\tLoss 1.4561 (1.0856)\tPrec@1 62.500 (75.891)\tPrec@5 89.844 (92.657)\n",
      "Test: [300/391]\tTime 1.256 (1.267)\tLoss 1.1162 (1.0908)\tPrec@1 71.094 (75.722)\tPrec@5 92.188 (92.616)\n",
      "Test: [310/391]\tTime 1.256 (1.267)\tLoss 1.7109 (1.0889)\tPrec@1 59.375 (75.746)\tPrec@5 86.719 (92.635)\n",
      "Test: [320/391]\tTime 1.252 (1.267)\tLoss 1.0615 (1.0867)\tPrec@1 71.875 (75.740)\tPrec@5 92.969 (92.672)\n",
      "Test: [330/391]\tTime 1.253 (1.266)\tLoss 0.9673 (1.0829)\tPrec@1 78.125 (75.810)\tPrec@5 94.531 (92.726)\n",
      "Test: [340/391]\tTime 1.258 (1.266)\tLoss 1.0488 (1.0808)\tPrec@1 80.469 (75.882)\tPrec@5 92.188 (92.756)\n",
      "Test: [350/391]\tTime 1.254 (1.266)\tLoss 0.7002 (1.0766)\tPrec@1 84.375 (75.950)\tPrec@5 98.438 (92.808)\n",
      "Test: [360/391]\tTime 1.255 (1.265)\tLoss 0.6680 (1.0709)\tPrec@1 90.625 (76.041)\tPrec@5 96.094 (92.899)\n",
      "Test: [370/391]\tTime 1.256 (1.265)\tLoss 1.1533 (1.0684)\tPrec@1 76.562 (76.080)\tPrec@5 93.750 (92.954)\n",
      "Test: [380/391]\tTime 1.253 (1.265)\tLoss 0.7910 (1.0678)\tPrec@1 79.688 (76.111)\tPrec@5 96.094 (92.963)\n",
      "Test: [390/391]\tTime 3.763 (1.271)\tLoss 1.2871 (1.0695)\tPrec@1 70.000 (76.080)\tPrec@5 86.250 (92.944)\n",
      "~~0\t0.14276541305555554\t92.944\n",
      "\n",
      " * Prec@1 76.080 Prec@5 92.944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.08"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tta(val_loader, aug_loader, model, criterion, 0, start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TTA with size*1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/391]\tTime 6.244 (6.244)\tLoss 1.2451 (1.2451)\tPrec@1 72.656 (72.656)\tPrec@5 92.969 (92.969)\n",
      "Test: [10/391]\tTime 1.191 (1.676)\tLoss 1.1172 (1.0119)\tPrec@1 79.688 (76.989)\tPrec@5 92.969 (93.821)\n",
      "Test: [20/391]\tTime 1.205 (1.448)\tLoss 1.1221 (1.1017)\tPrec@1 77.344 (75.298)\tPrec@5 93.750 (92.857)\n",
      "Test: [30/391]\tTime 1.207 (1.366)\tLoss 1.0488 (1.0948)\tPrec@1 80.469 (75.529)\tPrec@5 91.406 (93.095)\n",
      "Test: [40/391]\tTime 1.215 (1.324)\tLoss 1.2266 (1.0721)\tPrec@1 70.312 (75.953)\tPrec@5 92.188 (93.388)\n",
      "Test: [50/391]\tTime 1.195 (1.300)\tLoss 1.3477 (1.1382)\tPrec@1 71.875 (75.046)\tPrec@5 89.062 (92.601)\n",
      "Test: [60/391]\tTime 1.196 (1.284)\tLoss 2.0469 (1.1921)\tPrec@1 57.031 (74.091)\tPrec@5 84.375 (92.059)\n",
      "Test: [70/391]\tTime 1.201 (1.272)\tLoss 1.3076 (1.1899)\tPrec@1 71.094 (74.120)\tPrec@5 90.625 (92.077)\n",
      "Test: [80/391]\tTime 1.205 (1.263)\tLoss 1.1348 (1.1628)\tPrec@1 76.562 (74.605)\tPrec@5 90.625 (92.371)\n",
      "Test: [90/391]\tTime 1.212 (1.258)\tLoss 1.2207 (1.1519)\tPrec@1 75.000 (74.845)\tPrec@5 90.625 (92.402)\n",
      "Test: [100/391]\tTime 1.217 (1.254)\tLoss 0.9790 (1.1303)\tPrec@1 76.562 (75.116)\tPrec@5 94.531 (92.644)\n",
      "Test: [110/391]\tTime 1.250 (1.253)\tLoss 1.3613 (1.1232)\tPrec@1 71.875 (75.282)\tPrec@5 87.500 (92.673)\n",
      "Test: [120/391]\tTime 1.250 (1.253)\tLoss 1.4580 (1.1459)\tPrec@1 71.875 (74.774)\tPrec@5 88.281 (92.336)\n",
      "Test: [130/391]\tTime 1.257 (1.253)\tLoss 0.9834 (1.1336)\tPrec@1 75.781 (74.845)\tPrec@5 92.188 (92.396)\n",
      "Test: [140/391]\tTime 1.252 (1.253)\tLoss 1.0430 (1.1187)\tPrec@1 75.000 (74.983)\tPrec@5 89.844 (92.509)\n",
      "Test: [150/391]\tTime 1.252 (1.253)\tLoss 0.6553 (1.1094)\tPrec@1 85.156 (75.222)\tPrec@5 96.094 (92.565)\n",
      "Test: [160/391]\tTime 1.252 (1.253)\tLoss 1.1943 (1.0939)\tPrec@1 72.656 (75.587)\tPrec@5 92.969 (92.663)\n",
      "Test: [170/391]\tTime 1.252 (1.253)\tLoss 0.6670 (1.0863)\tPrec@1 82.812 (75.763)\tPrec@5 96.875 (92.722)\n",
      "Test: [180/391]\tTime 1.251 (1.253)\tLoss 0.4680 (1.0773)\tPrec@1 89.844 (75.928)\tPrec@5 98.438 (92.783)\n",
      "Test: [190/391]\tTime 1.252 (1.253)\tLoss 0.6494 (1.0780)\tPrec@1 85.156 (75.924)\tPrec@5 95.312 (92.785)\n",
      "Test: [200/391]\tTime 1.252 (1.253)\tLoss 1.3105 (1.0650)\tPrec@1 64.844 (76.174)\tPrec@5 93.750 (92.918)\n",
      "Test: [210/391]\tTime 1.253 (1.253)\tLoss 0.9775 (1.0579)\tPrec@1 75.000 (76.307)\tPrec@5 93.750 (93.013)\n",
      "Test: [220/391]\tTime 1.264 (1.253)\tLoss 0.6689 (1.0505)\tPrec@1 86.719 (76.361)\tPrec@5 93.750 (93.160)\n",
      "Test: [230/391]\tTime 1.254 (1.253)\tLoss 0.6372 (1.0437)\tPrec@1 85.156 (76.546)\tPrec@5 97.656 (93.233)\n",
      "Test: [240/391]\tTime 1.251 (1.253)\tLoss 1.4580 (1.0565)\tPrec@1 60.938 (76.251)\tPrec@5 91.406 (93.121)\n",
      "Test: [250/391]\tTime 1.253 (1.253)\tLoss 1.1895 (1.0662)\tPrec@1 75.781 (76.055)\tPrec@5 91.406 (93.047)\n",
      "Test: [260/391]\tTime 1.253 (1.254)\tLoss 1.5791 (1.0748)\tPrec@1 64.062 (75.946)\tPrec@5 84.375 (92.909)\n",
      "Test: [270/391]\tTime 1.255 (1.253)\tLoss 1.2773 (1.0805)\tPrec@1 70.312 (75.827)\tPrec@5 90.625 (92.874)\n",
      "Test: [280/391]\tTime 1.254 (1.254)\tLoss 1.6094 (1.0878)\tPrec@1 71.094 (75.690)\tPrec@5 82.812 (92.735)\n",
      "Test: [290/391]\tTime 1.255 (1.254)\tLoss 1.4990 (1.0983)\tPrec@1 65.625 (75.499)\tPrec@5 88.281 (92.604)\n",
      "Test: [300/391]\tTime 1.242 (1.254)\tLoss 1.1094 (1.1033)\tPrec@1 71.094 (75.340)\tPrec@5 94.531 (92.572)\n",
      "Test: [310/391]\tTime 1.256 (1.254)\tLoss 1.7754 (1.1013)\tPrec@1 57.031 (75.374)\tPrec@5 83.594 (92.587)\n",
      "Test: [320/391]\tTime 1.252 (1.254)\tLoss 1.0527 (1.0984)\tPrec@1 75.781 (75.450)\tPrec@5 91.406 (92.626)\n",
      "Test: [330/391]\tTime 1.264 (1.254)\tLoss 0.9985 (1.0948)\tPrec@1 76.562 (75.500)\tPrec@5 94.531 (92.676)\n",
      "Test: [340/391]\tTime 1.256 (1.254)\tLoss 1.0879 (1.0923)\tPrec@1 78.125 (75.570)\tPrec@5 92.969 (92.710)\n",
      "Test: [350/391]\tTime 1.251 (1.254)\tLoss 0.6807 (1.0882)\tPrec@1 84.375 (75.657)\tPrec@5 98.438 (92.753)\n",
      "Test: [360/391]\tTime 1.254 (1.254)\tLoss 0.6006 (1.0819)\tPrec@1 89.062 (75.749)\tPrec@5 96.094 (92.832)\n",
      "Test: [370/391]\tTime 1.265 (1.254)\tLoss 1.1934 (1.0795)\tPrec@1 76.562 (75.798)\tPrec@5 94.531 (92.882)\n",
      "Test: [380/391]\tTime 1.259 (1.254)\tLoss 0.7944 (1.0786)\tPrec@1 81.250 (75.828)\tPrec@5 95.312 (92.897)\n",
      "Test: [390/391]\tTime 0.797 (1.253)\tLoss 1.3418 (1.0790)\tPrec@1 71.250 (75.818)\tPrec@5 85.000 (92.904)\n",
      "~~0\t0.37358359\t92.904\n",
      "\n",
      " * Prec@1 75.818 Prec@5 92.904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.818"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_scale = 0.5\n",
    "trn_tfms = [\n",
    "        transforms.Resize(int(target_size*1.14)),\n",
    "        transforms.RandomResizedCrop(args.sz, scale=(min_scale, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tensor_tfm\n",
    "aug_dataset = datasets.ImageFolder(valdir, transforms.Compose(trn_tfms))\n",
    "\n",
    "aug_loader = torch.utils.data.DataLoader(\n",
    "    aug_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "tta(val_loader, aug_loader, model, criterion, 0, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/391]\tTime 6.210 (6.210)\tLoss 1.2402 (1.2402)\tPrec@1 71.875 (71.875)\tPrec@5 91.406 (91.406)\n",
      "Test: [10/391]\tTime 1.219 (1.674)\tLoss 0.9854 (0.9604)\tPrec@1 80.469 (75.852)\tPrec@5 93.750 (93.395)\n",
      "Test: [20/391]\tTime 1.205 (1.452)\tLoss 1.0918 (1.0474)\tPrec@1 77.344 (74.628)\tPrec@5 92.969 (92.411)\n",
      "Test: [30/391]\tTime 1.256 (1.375)\tLoss 0.9312 (1.0464)\tPrec@1 82.812 (75.000)\tPrec@5 91.406 (92.591)\n",
      "Test: [40/391]\tTime 1.250 (1.341)\tLoss 1.1113 (1.0226)\tPrec@1 71.875 (75.419)\tPrec@5 93.750 (92.931)\n",
      "Test: [50/391]\tTime 1.251 (1.324)\tLoss 1.3223 (1.0860)\tPrec@1 67.969 (74.418)\tPrec@5 90.625 (92.096)\n",
      "Test: [60/391]\tTime 1.259 (1.313)\tLoss 1.9434 (1.1426)\tPrec@1 57.812 (73.335)\tPrec@5 82.812 (91.496)\n",
      "Test: [70/391]\tTime 1.260 (1.305)\tLoss 1.2363 (1.1423)\tPrec@1 71.875 (73.404)\tPrec@5 89.062 (91.549)\n",
      "Test: [80/391]\tTime 1.252 (1.299)\tLoss 1.1074 (1.1156)\tPrec@1 73.438 (73.852)\tPrec@5 89.062 (91.850)\n",
      "Test: [90/391]\tTime 1.251 (1.294)\tLoss 1.1143 (1.1054)\tPrec@1 75.781 (74.124)\tPrec@5 94.531 (91.973)\n",
      "Test: [100/391]\tTime 1.261 (1.290)\tLoss 0.9976 (1.0871)\tPrec@1 78.906 (74.443)\tPrec@5 94.531 (92.226)\n",
      "Test: [110/391]\tTime 1.253 (1.287)\tLoss 1.3662 (1.0847)\tPrec@1 67.969 (74.514)\tPrec@5 85.938 (92.237)\n",
      "Test: [120/391]\tTime 1.251 (1.284)\tLoss 1.4912 (1.1134)\tPrec@1 67.969 (74.051)\tPrec@5 85.156 (91.858)\n",
      "Test: [130/391]\tTime 1.251 (1.282)\tLoss 0.9219 (1.1028)\tPrec@1 77.344 (74.147)\tPrec@5 90.625 (91.943)\n",
      "Test: [140/391]\tTime 1.251 (1.280)\tLoss 1.0645 (1.0889)\tPrec@1 76.562 (74.396)\tPrec@5 91.406 (92.082)\n",
      "Test: [150/391]\tTime 1.251 (1.278)\tLoss 0.6284 (1.0822)\tPrec@1 84.375 (74.576)\tPrec@5 94.531 (92.115)\n",
      "Test: [160/391]\tTime 1.265 (1.277)\tLoss 1.2549 (1.0678)\tPrec@1 72.656 (74.942)\tPrec@5 91.406 (92.226)\n",
      "Test: [170/391]\tTime 1.264 (1.276)\tLoss 0.6646 (1.0613)\tPrec@1 83.594 (75.142)\tPrec@5 96.094 (92.320)\n",
      "Test: [180/391]\tTime 1.252 (1.274)\tLoss 0.4446 (1.0524)\tPrec@1 87.500 (75.324)\tPrec@5 97.656 (92.364)\n",
      "Test: [190/391]\tTime 1.251 (1.273)\tLoss 0.6387 (1.0545)\tPrec@1 84.375 (75.290)\tPrec@5 94.531 (92.351)\n",
      "Test: [200/391]\tTime 1.251 (1.272)\tLoss 1.2549 (1.0417)\tPrec@1 69.531 (75.614)\tPrec@5 93.750 (92.479)\n",
      "Test: [210/391]\tTime 1.247 (1.271)\tLoss 0.9795 (1.0354)\tPrec@1 74.219 (75.729)\tPrec@5 94.531 (92.610)\n",
      "Test: [220/391]\tTime 1.251 (1.271)\tLoss 0.6499 (1.0293)\tPrec@1 86.719 (75.746)\tPrec@5 93.750 (92.753)\n",
      "Test: [230/391]\tTime 1.251 (1.270)\tLoss 0.6636 (1.0232)\tPrec@1 82.812 (75.971)\tPrec@5 96.094 (92.810)\n",
      "Test: [240/391]\tTime 1.252 (1.269)\tLoss 1.3838 (1.0355)\tPrec@1 60.938 (75.707)\tPrec@5 89.844 (92.671)\n",
      "Test: [250/391]\tTime 1.251 (1.269)\tLoss 1.1406 (1.0443)\tPrec@1 75.000 (75.517)\tPrec@5 87.500 (92.586)\n",
      "Test: [260/391]\tTime 1.253 (1.268)\tLoss 1.6201 (1.0529)\tPrec@1 64.062 (75.401)\tPrec@5 82.812 (92.439)\n",
      "Test: [270/391]\tTime 1.252 (1.268)\tLoss 1.1924 (1.0583)\tPrec@1 71.875 (75.274)\tPrec@5 92.188 (92.415)\n",
      "Test: [280/391]\tTime 1.251 (1.268)\tLoss 1.6074 (1.0656)\tPrec@1 68.750 (75.128)\tPrec@5 82.812 (92.260)\n",
      "Test: [290/391]\tTime 1.250 (1.267)\tLoss 1.4453 (1.0750)\tPrec@1 64.062 (74.962)\tPrec@5 89.062 (92.147)\n",
      "Test: [300/391]\tTime 1.267 (1.267)\tLoss 1.1348 (1.0801)\tPrec@1 68.750 (74.761)\tPrec@5 94.531 (92.110)\n",
      "Test: [310/391]\tTime 1.251 (1.266)\tLoss 1.7168 (1.0783)\tPrec@1 56.250 (74.812)\tPrec@5 84.375 (92.117)\n",
      "Test: [320/391]\tTime 1.251 (1.266)\tLoss 1.0625 (1.0757)\tPrec@1 72.656 (74.871)\tPrec@5 92.969 (92.178)\n",
      "Test: [330/391]\tTime 1.256 (1.266)\tLoss 0.9517 (1.0709)\tPrec@1 74.219 (74.941)\tPrec@5 92.188 (92.228)\n",
      "Test: [340/391]\tTime 1.251 (1.266)\tLoss 1.0098 (1.0682)\tPrec@1 76.562 (75.023)\tPrec@5 92.188 (92.268)\n",
      "Test: [350/391]\tTime 1.251 (1.265)\tLoss 0.5913 (1.0634)\tPrec@1 86.719 (75.120)\tPrec@5 98.438 (92.321)\n",
      "Test: [360/391]\tTime 1.251 (1.265)\tLoss 0.6396 (1.0571)\tPrec@1 90.625 (75.212)\tPrec@5 96.094 (92.410)\n",
      "Test: [370/391]\tTime 1.271 (1.265)\tLoss 1.0498 (1.0544)\tPrec@1 78.906 (75.244)\tPrec@5 93.750 (92.459)\n",
      "Test: [380/391]\tTime 1.250 (1.265)\tLoss 0.7539 (1.0526)\tPrec@1 81.250 (75.262)\tPrec@5 94.531 (92.483)\n",
      "Test: [390/391]\tTime 0.798 (1.263)\tLoss 1.2998 (1.0536)\tPrec@1 70.000 (75.236)\tPrec@5 85.000 (92.474)\n",
      "~~0\t0.5196896316666667\t92.474\n",
      "\n",
      " * Prec@1 75.236 Prec@5 92.474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.236"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_scale = 1\n",
    "trn_tfms = [\n",
    "        transforms.Resize(int(target_size*1.14)),\n",
    "        transforms.RandomResizedCrop(args.sz, scale=(min_scale, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ] + tensor_tfm\n",
    "aug_dataset = datasets.ImageFolder(valdir, transforms.Compose(trn_tfms))\n",
    "\n",
    "aug_loader = torch.utils.data.DataLoader(\n",
    "    aug_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "tta(val_loader, aug_loader, model, criterion, 0, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_scale = 0.5\n",
    "aug_tfms = [\n",
    "        transforms.Resize(int(target_size*1.14)),\n",
    "        RandomCropArTfm(idx2ar, target_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ]\n",
    "aug_dataset = ValDataset(valdir, aug_tfms+tensor_tfm)\n",
    "\n",
    "aug_loader = torch.utils.data.DataLoader(\n",
    "    aug_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta(val_loader, aug_loader, model, criterion, 0, start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
