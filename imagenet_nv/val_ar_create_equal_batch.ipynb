{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, shutil, time, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "# import models\n",
    "from fp16util import *\n",
    "import gc\n",
    "\n",
    "import resnet\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "# import resnet_sd as resnet\n",
    "\n",
    "# model_names = sorted(name for name in models.__dict__\n",
    "#                      if name.islower() and not name.startswith(\"__\")\n",
    "#                      and callable(models.__dict__[name]))\n",
    "#print(model_names)\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "    parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
    "    parser.add_argument('--save-dir', type=str, default=Path.cwd(), help='Directory to save logs and models.')\n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet50')\n",
    "    # parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet18',\n",
    "    #                     choices=model_names,\n",
    "    #                     help='model architecture: ' +\n",
    "    #                     ' | '.join(model_names) +\n",
    "    #                     ' (default: resnet18)')\n",
    "    parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--epochs', default=90, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('--warmup', default=0, type=int, metavar='N',\n",
    "                        help='number of additional epochs to warmup')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 256)')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                        metavar='N', help='print frequency (default: 10)')\n",
    "    parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--small', action='store_true', help='start with smaller images')\n",
    "    parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                        help='evaluate model on validation set')\n",
    "    parser.add_argument('--pretrained', dest='pretrained', action='store_true', help='use pre-trained model')\n",
    "    parser.add_argument('--fp16', action='store_true', help='Run model fp16 mode.')\n",
    "    parser.add_argument('--sz',       default=224, type=int, help='Size of transformed image.')\n",
    "    parser.add_argument('--decay-int', default=30, type=int, help='Decay LR by 10 every decay-int epochs')\n",
    "    parser.add_argument('--loss-scale', type=float, default=1,\n",
    "                        help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "    parser.add_argument('--prof', dest='prof', action='store_true', help='Only run a few iters for profiling.')\n",
    "    parser.add_argument('--val-ar', action='store_true', help='Do final validation by nearest aspect ratio')\n",
    "\n",
    "    parser.add_argument('--distributed', action='store_true', help='Run distributed training')\n",
    "    parser.add_argument('--dist-url', default='file://sync.file', type=str,\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=0, type=int,\n",
    "                        help='Used for multi-process training. Can either be manually set ' +\n",
    "                        'or automatically set by using \\'python -m multiproc\\'.')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "args = get_parser().parse_args(['/home/paperspace/data/imagenet', '--val-ar'])\n",
    "if args.local_rank > 0: sys.stdout = open(f'{args.save_dir}/GPU_{args.local_rank}.log', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_collate(batch):\n",
    "    if not batch: return torch.tensor([]), torch.tensor([])\n",
    "    imgs = [img[0] for img in batch]\n",
    "    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n",
    "    w = imgs[0].size[0]\n",
    "    h = imgs[0].size[1]\n",
    "    tensor = torch.zeros( (len(imgs), 3, h, w), dtype=torch.uint8 )\n",
    "    for i, img in enumerate(imgs):\n",
    "        nump_array = np.asarray(img, dtype=np.uint8)\n",
    "        tens = torch.from_numpy(nump_array)\n",
    "        if(nump_array.ndim < 3):\n",
    "            nump_array = np.expand_dims(nump_array, axis=-1)\n",
    "        nump_array = np.rollaxis(nump_array, 2)\n",
    "\n",
    "        tensor[i] += torch.from_numpy(nump_array)\n",
    "        \n",
    "    return tensor, targets\n",
    "\n",
    "\n",
    "import os.path\n",
    "def sort_ar(valdir):\n",
    "    idx2ar_file = args.data+'/sorted_idxar.p'\n",
    "    if os.path.isfile(idx2ar_file): return pickle.load(open(idx2ar_file, 'rb'))\n",
    "    print('Creating AR indexes. Please be patient this may take a couple minutes...')\n",
    "    val_dataset = datasets.ImageFolder(valdir)\n",
    "    sizes = [img[0].size for img in tqdm(val_dataset, total=len(val_dataset))]\n",
    "    idx_ar = [(i, round(s[0]/s[1], 5)) for i,s in enumerate(sizes)]\n",
    "    sorted_idxar = sorted(idx_ar, key=lambda x: x[1])\n",
    "    pickle.dump(sorted_idxar, open(idx2ar_file, 'wb'))\n",
    "    print('Done')\n",
    "    return sorted_idxar\n",
    "\n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "\n",
    "def map_idx2ar(idx_ar_sorted, batch_size):\n",
    "    # idx2ar_map_file = args.data+f'/idxar_map_{batch_size}.p'\n",
    "    # if os.path.isfile(idx2ar_map_file): return pickle.load(open(idx2ar_map_file, 'rb'))\n",
    "    ar_chunks = list(chunks(idx_ar_sorted, batch_size))\n",
    "    idx2ar = {}\n",
    "    for chunk in ar_chunks:\n",
    "        idxs, ars = list(zip(*chunk))\n",
    "        mean = round(np.mean(ars), 5)\n",
    "        for idx in idxs:\n",
    "            idx2ar[idx] = mean\n",
    "    # pickle.dump(idx2ar, open(idx2ar_map_file, 'wb'))\n",
    "    return idx2ar\n",
    "\n",
    "class ValDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            for tfm in self.transform:\n",
    "                if isinstance(tfm, CropArTfm): sample = tfm(sample, index)\n",
    "                else: sample = tfm(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    \n",
    "class BatchSampler(object):\n",
    "    def __init__(self, sampler, batch_size, drop_last):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.sampler:\n",
    "            batch.append(int(idx))\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "class ValDistSampler(Sampler):\n",
    "    def __init__(self, indices, min_batch_size=None):\n",
    "        self.indices = indices\n",
    "        if args.distributed: \n",
    "            self.world_size = dist.get_world_size() \n",
    "            self.rank = dist.get_rank()\n",
    "        else: \n",
    "            self.rank = 0\n",
    "            self.world_size = 1\n",
    "        if preserve_batch_size: \n",
    "            self.num_samples = math.ceil(len(self.indices) / self.world_size / min_batch_size) * min_batch_size\n",
    "        else:\n",
    "            self.num_samples = int(math.ceil(len(self.indices) * 1.0 / self.world_size))\n",
    "    def __iter__(self):\n",
    "        offset = self.num_samples * self.rank\n",
    "        return iter(self.indices[offset:offset+self.num_samples])\n",
    "    def __len__(self): return self.num_samples\n",
    "    def set_epoch(self, epoch): return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3,4][5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ValDistSampler(Sampler):\n",
    "    # min_batch_size - validation by nearest aspect ratio expects the batch size to be constant\n",
    "    # Otherwise you'll mix different images with different aspect ratio's and tensor will not be constant size\n",
    "    def __init__(self, indices, batch_size, distributed_batch=True):\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "        if distributed_batch: \n",
    "            self.world_size = dist.get_world_size() \n",
    "            self.rank = dist.get_rank()\n",
    "        else: \n",
    "            self.rank = 0\n",
    "            self.world_size = 1\n",
    "            \n",
    "        # expected number of batches per sample. Need this so each distributed gpu validates on same number of batches.\n",
    "        # even if there isn't enough data to go around\n",
    "        self.expected_num_batches = math.ceil(len(self.indices) / self.world_size / self.batch_size)\n",
    "        \n",
    "        # num_samples = total images / world_size. This is what we distribute to each gpu\n",
    "        self.num_samples = self.expected_num_batches * self.batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        offset = self.num_samples * self.rank\n",
    "        sampled_indices = self.indices[offset:offset+self.num_samples]\n",
    "        for i in range(self.expected_num_batches):\n",
    "            offset = i*self.batch_size\n",
    "            yield sampled_indices[offset:offset+self.batch_size]\n",
    "    def __len__(self): return self.expected_num_batches\n",
    "    def set_epoch(self, epoch): return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 50000\n",
    "bs = 64\n",
    "ws = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(tot/ws/bs)*bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1600*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeBatchSampler(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx in range(10):\n",
    "            yield []\n",
    "        batch = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_sampler = FakeBatchSampler()\n",
    "traindir = args.data+'/train'\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir, transforms.Compose([\n",
    "        transforms.RandomResizedCrop(128, scale=1),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ]))\n",
    "# train_sampler = (torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, #batch_size=bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, collate_fn=fast_collate, \n",
    "    batch_sampler=fake_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([]), tensor([]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing new validation dist sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestValDistSampler(Sampler):\n",
    "    # min_batch_size - validation by nearest aspect ratio expects the batch size to be constant\n",
    "    # Otherwise you'll mix different images with different aspect ratio's and tensor will not be constant size\n",
    "    def __init__(self, indices, batch_size, world_size, rank):\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        # expected number of batches per sample. Need this so each distributed gpu validates on same number of batches.\n",
    "        # even if there isn't enough data to go around\n",
    "        self.expected_num_batches = math.ceil(len(self.indices) / self.world_size / batch_size)\n",
    "        \n",
    "        # num_samples = total images / world_size. This is what we distribute to each gpu\n",
    "        self.num_samples = self.expected_num_batches * batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        offset = self.num_samples * self.rank\n",
    "        sampled_indices = self.indices[offset:offset+self.num_samples]\n",
    "        for i in range(self.expected_num_batches):\n",
    "            offset = i*self.batch_size\n",
    "            yield sampled_indices[offset:offset+self.batch_size]\n",
    "    def __len__(self): return self.expected_num_batches\n",
    "    def set_epoch(self, epoch): return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "target_size = 128\n",
    "valdir = args.data+'/validation'\n",
    "idx_ar_sorted = sort_ar(valdir)\n",
    "idx_sorted, _ = zip(*idx_ar_sorted)\n",
    "idx2ar = map_idx2ar(idx_ar_sorted, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_tfms = [transforms.Resize(int(target_size*1.14)), CropArTfm(idx2ar, target_size)]\n",
    "val_dataset = ValDataset(valdir, transform=ar_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sampler = TestValDistSampler(idx_sorted, batch_size, 32, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, collate_fn=fast_collate, \n",
    "    batch_sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([]), tensor([])]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropArTfm(object):\n",
    "    def __init__(self, idx2ar, target_size):\n",
    "        self.idx2ar, self.target_size = idx2ar, target_size\n",
    "    def __call__(self, img, idx):\n",
    "        target_ar = self.idx2ar[idx]\n",
    "        if target_ar < 1: \n",
    "            w = int(self.target_size/target_ar)\n",
    "            size = (w//8*8, self.target_size)\n",
    "        else: \n",
    "            h = int(self.target_size*target_ar)\n",
    "            size = (self.target_size, h//8*8)\n",
    "        return torchvision.transforms.functional.center_crop(img, size)\n",
    "\n",
    "def create_validation_set(valdir, batch_size, target_size, use_ar):\n",
    "    idx_ar_sorted = sort_ar(valdir)\n",
    "    idx_sorted, _ = zip(*idx_ar_sorted)\n",
    "    idx2ar = map_idx2ar(idx_ar_sorted, batch_size)\n",
    "    \n",
    "    if use_ar:\n",
    "        ar_tfms = [transforms.Resize(int(target_size*1.14)), CropArTfm(idx2ar, target_size)]\n",
    "        val_dataset = ValDataset(valdir, transform=ar_tfms)\n",
    "        val_sampler = ValDistSampler(idx_sorted)\n",
    "        return val_dataset, val_sampler\n",
    "    \n",
    "    val_tfms = [transforms.Resize(int(args.sz*1.14)), transforms.CenterCrop(args.sz)]\n",
    "    val_dataset = datasets.ImageFolder(valdir, transforms.Compose(val_tfms))\n",
    "    val_sampler = ValDistSampler(list(range(len(val_dataset))))\n",
    "    return val_dataset, val_sampler\n",
    "\n",
    "def get_loaders(traindir, valdir, sz, bs, val_bs=None, use_ar=False, min_scale=0.08):\n",
    "    val_bs = val_bs or bs\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        traindir, transforms.Compose([\n",
    "            transforms.RandomResizedCrop(sz, scale=(min_scale, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ]))\n",
    "    train_sampler = (torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True, collate_fn=fast_collate, \n",
    "        sampler=train_sampler)\n",
    "\n",
    "    val_dataset, val_sampler = create_validation_set(valdir, val_bs, sz, use_ar=use_ar)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True, collate_fn=fast_collate, \n",
    "        sampler=val_sampler)\n",
    "    print('Val_bs:', val_bs)\n",
    "    return train_loader,val_loader,train_sampler,val_sampler\n",
    "\n",
    "\n",
    "# Seems to speed up training by ~2%\n",
    "class DataPrefetcher():\n",
    "    def __init__(self, loader, stop_after=None, prefetch=True):\n",
    "        self.loader = loader\n",
    "        # self.dataset = loader.dataset\n",
    "        self.prefetch = prefetch\n",
    "        if prefetch:\n",
    "            self.stream = torch.cuda.Stream()\n",
    "            self.stop_after = stop_after\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "\n",
    "            self.mean = torch.tensor([0.485 * 255, 0.456 * 255, 0.406 * 255]).cuda().view(1,3,1,1)\n",
    "            self.std = torch.tensor([0.229 * 255, 0.224 * 255, 0.225 * 255]).cuda().view(1,3,1,1)\n",
    "            if args.fp16:\n",
    "                self.mean = self.mean.half()\n",
    "                self.std = self.std.half()\n",
    "\n",
    "    def __len__(self): return len(self.loader)\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loaditer)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "            if args.fp16: self.next_input = self.next_input.half()\n",
    "            else: self.next_input = self.next_input.float()\n",
    "            self.next_input = self.next_input.sub_(self.mean).div_(self.std)\n",
    "            \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        self.loaditer = iter(self.loader)\n",
    "        if not self.prefetch: return self.load_iter\n",
    "        self.preload()\n",
    "        while self.next_input is not None:\n",
    "            torch.cuda.current_stream().wait_stream(self.stream)\n",
    "            input = self.next_input\n",
    "            target = self.next_target\n",
    "            self.preload()\n",
    "            # if count == 0: print('Prefetcher first preload complete')\n",
    "            count += 1\n",
    "            yield input, target\n",
    "            if type(self.stop_after) is int and (count > self.stop_after):\n",
    "                break\n",
    "\n",
    "class DataManager():\n",
    "    def __init__(self):\n",
    "        if args.small: self.load_data('-sz/160', args.batch_size, 128)\n",
    "        # else: self.load_data('-sz/320', args.batch_size, 224)\n",
    "        else: self.load_data('', args.batch_size, 224)\n",
    "        \n",
    "    def set_epoch(self, epoch):\n",
    "        if epoch==int(args.epochs*0.4+0.5)+args.warmup:\n",
    "            # self.load_data('-sz/320', args.batch_size, 224)\n",
    "            print('DataManager changing image size to 244')\n",
    "            self.load_data('', args.batch_size, 224)\n",
    "        if epoch==int(args.epochs*0.92+0.5)+args.warmup:\n",
    "            print('DataManager changing image size to 288')\n",
    "        self.load_data('', 128, 288, val_bs=64, min_scale=0.5, use_ar=args.val_ar)\n",
    "        if args.distributed:\n",
    "            if self.trn_smp: self.trn_smp.set_epoch(epoch)\n",
    "            if self.val_smp: self.val_smp.set_epoch(epoch)\n",
    "\n",
    "    def get_trn_iter(self):\n",
    "        # trn_iter = self.trn_iter\n",
    "        self.trn_iter = iter(self.trn_dl)\n",
    "        return self.trn_iter\n",
    "\n",
    "    def get_val_iter(self):\n",
    "        # val_iter = self.val_iter\n",
    "        self.val_iter = iter(self.val_dl)\n",
    "        return self.val_iter\n",
    "        \n",
    "    def load_data(self, dir_prefix, batch_size, image_size, **kwargs):\n",
    "        traindir = args.data+dir_prefix+'/train'\n",
    "        valdir = args.data+dir_prefix+'/validation'\n",
    "        self.trn_dl,self.val_dl,self.trn_smp,self.val_smp = get_loaders(traindir, valdir, bs=batch_size, sz=image_size, **kwargs)\n",
    "        self.trn_dl = DataPrefetcher(self.trn_dl)\n",
    "        self.val_dl = DataPrefetcher(self.val_dl)\n",
    "\n",
    "        self.trn_len = len(self.trn_dl)\n",
    "        self.val_len = len(self.val_dl)\n",
    "        # self.trn_iter = iter(self.trn_dl)\n",
    "        # self.val_iter = iter(self.val_dl)\n",
    "\n",
    "        # clear memory\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class Scheduler():\n",
    "    def __init__(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.current_lr = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def get_lr(self, epoch, batch_num, batch_tot):\n",
    "        \"\"\"Sets the learning rate to the initial LR decayed by 10 every few epochs\"\"\"\n",
    "        if epoch<int(args.epochs*0.14)+args.warmup:\n",
    "            epoch_tot = int(args.epochs*0.14)+args.warmup\n",
    "            world_size = dist.get_world_size()\n",
    "            # lr_step = (world_size/4 - 1) * args.lr / (epoch_tot * batch_tot)\n",
    "            lr_step = args.lr / (epoch_tot * batch_tot)\n",
    "            lr = args.lr/2 + (epoch * batch_tot + batch_num) * lr_step\n",
    "            # this should atually be args.lr / world_size + lr_step\n",
    "\n",
    "\n",
    "            # lr = args.lr\n",
    "\n",
    "        # the following works best for 8 machines I think\n",
    "        # if epoch<int(args.epochs*0.14)+args.warmup:\n",
    "        #     epoch_tot = int(args.epochs*0.14)+args.warmup\n",
    "        #     starting_lr = args.lr/epoch_tot\n",
    "        #     world_size = dist.get_world_size()\n",
    "        #     if (world_size > 20) and (epoch < 4):\n",
    "        #         # starting_lr = starting_lr/(world_size/2)\n",
    "        #         starting_lr = starting_lr/(4 - epoch)\n",
    "        #     ending_lr = args.lr\n",
    "        #     step_size = (ending_lr - starting_lr)/epoch_tot\n",
    "        #     batch_step_size = step_size/batch_tot\n",
    "        #     lr = step_size*epoch + batch_step_size*batch_num\n",
    "\n",
    "            # lr = args.lr/(int(args.epochs*0.1)+args.warmup-epoch)\n",
    "        elif epoch<int(args.epochs*0.47+0.5)+args.warmup: lr = args.lr/1\n",
    "        elif epoch<int(args.epochs*0.78+0.5)+args.warmup: lr = args.lr/10\n",
    "        elif epoch<int(args.epochs*0.95+0.5)+args.warmup: lr = args.lr/95\n",
    "        else         : lr = args.lr/1000\n",
    "        return lr\n",
    "\n",
    "    def update_lr(self, epoch, batch_num, batch_tot):\n",
    "        lr = self.get_lr(epoch, batch_num, batch_tot)\n",
    "        if (self.current_lr != lr) and ((batch_num == 0) or (batch_num+1 == batch_tot)): \n",
    "            print(f'Changing LR from {self.current_lr} to {lr}')\n",
    "\n",
    "        self.current_lr = lr\n",
    "        self.current_epoch = epoch\n",
    "        self.current_batch = batch_num\n",
    "\n",
    "        for param_group in self.optimizer.param_groups: param_group['lr'] = lr\n",
    "\n",
    "        if not args.distributed: return\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            lr_old = param_group['lr']\n",
    "            param_group['lr'] = lr\n",
    "            # Trick 4: apply momentum correction when lr is updated\n",
    "            if lr > lr_old:\n",
    "                param_group['momentum'] = lr / lr_old * args.momentum\n",
    "            else:\n",
    "                param_group['momentum'] = args.momentum\n",
    "\n",
    "def init_dist_weights(model):\n",
    "    # Distributed training uses 4 tricks to maintain the accuracy\n",
    "    # with much larger batchsize, see\n",
    "    # https://arxiv.org/pdf/1706.02677.pdf\n",
    "    # for more details\n",
    "\n",
    "    if args.arch.startswith('resnet'):\n",
    "        for m in model.modules():\n",
    "            # Trick 1: the last BatchNorm layer in each block need to\n",
    "            # be initialized as zero gamma\n",
    "            if isinstance(m, resnet.BasicBlock):\n",
    "                m.bn2.weight = Parameter(torch.zeros_like(m.bn2.weight))\n",
    "            if isinstance(m, resnet.Bottleneck):\n",
    "                m.bn3.weight = Parameter(torch.zeros_like(m.bn3.weight))\n",
    "            # Trick 2: linear layers are initialized by\n",
    "            # drawing weights from a zero-mean Gaussian with\n",
    "            # standard deviation of 0.01. In the paper it was only\n",
    "            # fc layer, but in practice we found this better for\n",
    "            # accuracy.\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# item() is a recent addition, so this helps with backward compatibility.\n",
    "def to_python_float(t):\n",
    "    if hasattr(t, 'item'):\n",
    "        return t.item()\n",
    "    else:\n",
    "        return t[0]\n",
    "\n",
    "def train(trn_iter, trn_len, model, criterion, optimizer, scheduler, epoch, base_model):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    st = time.time()\n",
    "    # print('Begin training loop:', st)\n",
    "    for i,(input,target) in enumerate(trn_iter):\n",
    "        # if i == 0: print('Received input:', time.time()-st)\n",
    "        if args.prof and (i > 200): break\n",
    "        if hasattr(base_model, 'rseed'):\n",
    "            if epoch<int(args.epochs*0.14)+args.warmup: base_model.rseed = -1\n",
    "            else: base_model.rseed = 1000*epoch+i\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        scheduler.update_lr(epoch, i, trn_len)\n",
    "\n",
    "        # input_var = Variable(input)\n",
    "        # target_var = Variable(target)\n",
    "        input_var = input\n",
    "        target_var = target\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "        if args.distributed:\n",
    "            reduced_loss = reduce_tensor(loss.data)\n",
    "            prec1 = reduce_tensor(prec1)\n",
    "            prec5 = reduce_tensor(prec5)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "\n",
    "        losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "        top1.update(to_python_float(prec1), input.size(0))\n",
    "        top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        loss = loss*args.loss_scale\n",
    "        # compute gradient and do SGD step\n",
    "        # if i == 0: print('Evaluate and loss:', time.time()-st)\n",
    "\n",
    "        if args.fp16:\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            model_grads_to_master_grads(model_params, master_params)\n",
    "\n",
    "            if args.loss_scale != 1:\n",
    "                for param in master_params:\n",
    "                    param.grad.data = param.grad.data/args.loss_scale\n",
    "\n",
    "            optimizer.step()\n",
    "            master_params_to_model_params(model_params, master_params)\n",
    "            torch.cuda.synchronize()\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # if i == 0: print('Backward step:', time.time()-st)\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        if args.local_rank == 0 and i % args.print_freq == 0 and i > 1:\n",
    "            \n",
    "            output = ('Epoch: [{0}][{1}/{2}]\\t' \\\n",
    "                    + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                    + 'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t' \\\n",
    "                    + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                    + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n",
    "                    + 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})').format(\n",
    "                    epoch, i, trn_len, batch_time=batch_time,\n",
    "                    data_time=data_time, loss=losses, top1=top1, top5=top5)\n",
    "            print(output)\n",
    "            with open(f'{args.save_dir}/full.log', 'a') as f:\n",
    "                f.write(output + '\\n')\n",
    "\n",
    "\n",
    "def validate(val_iter, val_len, model, criterion, epoch, start_time):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    for i,(input,target) in enumerate(val_iter):\n",
    "        # target = target.cuda(async=True)\n",
    "        # input_var = Variable(input)\n",
    "        # target_var = Variable(target)\n",
    "        input_var = input\n",
    "        target_var = target\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "        if args.distributed:\n",
    "            reduced_loss = reduce_tensor(loss.data)\n",
    "            prec1 = reduce_tensor(prec1)\n",
    "            prec5 = reduce_tensor(prec5)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "            \n",
    "\n",
    "        losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "        top1.update(to_python_float(prec1), input.size(0))\n",
    "        top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if args.local_rank == 0 and i % args.print_freq == 0:\n",
    "            output = ('Test: [{0}/{1}]\\t' \\\n",
    "                    + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                    + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                    + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n",
    "                    + 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})').format(\n",
    "                    i, val_len, batch_time=batch_time, loss=losses,\n",
    "                    top1=top1, top5=top5)\n",
    "            print(output)\n",
    "            with open(f'{args.save_dir}/full.log', 'a') as f:\n",
    "                f.write(output + '\\n')\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    print(f'~~{epoch}\\t{float(time_diff.total_seconds() / 3600.0)}\\t{top5.avg:.3f}\\n')\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return top5.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, f'{args.save_dir}/model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    size = dist.get_world_size()\n",
    "    # rt /= args.world_size\n",
    "    rt /= size\n",
    "    return rt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~epoch\thours\ttop1Accuracy\n",
      "\n",
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "print(\"~~epoch\\thours\\ttop1Accuracy\\n\")\n",
    "\n",
    "# need to index validation directory before we start counting the time\n",
    "if args.val_ar: sort_ar(args.data+'/validation')\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "if args.distributed:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url)\n",
    "    print('Distributed: init_process_group success')\n",
    "\n",
    "if args.fp16: assert torch.backends.cudnn.enabled, \"fp16 mode requires cudnn backend to be enabled.\"\n",
    "\n",
    "# create model\n",
    "# if args.pretrained: model = models.__dict__[args.arch](pretrained=True)\n",
    "# else: model = models.__dict__[args.arch]()\n",
    "# AS: force use resnet50 for now, until we figure out whether to upload model directory\n",
    "\n",
    "model = resnet.resnet50()\n",
    "# model = resnet.resnet68()\n",
    "base_model_pointer = model\n",
    "print(\"Loaded model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input has less dimensions than expected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-50cb38a93bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/imagenet-fast/imagenet_nv/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input has less dimensions than expected"
     ]
    }
   ],
   "source": [
    "model(torch.Tensor([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "n_dev = torch.cuda.device_count()\n",
    "if args.fp16: model = network_to_half(model)\n",
    "if args.distributed:\n",
    "    init_dist_weights(model) # (AS) Performs pretty poorly for first 10 epochs when enabled\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "\n",
    "global model_params, master_params\n",
    "if args.fp16:  model_params, master_params = prep_param_lists(model)\n",
    "else: master_params = list(model.parameters())\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(master_params, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "scheduler = Scheduler(optimizer)\n",
    "\n",
    "print(\"Defined loss and optimizer\")\n",
    "\n",
    "best_prec5 = 93 # only save models over 92%. Otherwise it stops to save every time\n",
    "# optionally resume from a checkpoint\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(args.gpu))\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    else: print(\"=> no checkpoint found at '{}'\".format(args.resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataManager()\n",
    "print(\"Created data loaders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataManager changing image size to 288\n",
      "Val_bs: 64\n",
      "Test: [0/782]\tTime 5.105 (5.105)\tLoss 84.6904 (84.6904)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Test: [10/782]\tTime 0.140 (1.638)\tLoss 82.5158 (84.0385)\tPrec@1 0.000 (0.000)\tPrec@5 1.562 (0.710)\n",
      "Test: [20/782]\tTime 0.140 (0.924)\tLoss 80.9901 (82.3959)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.670)\n",
      "Test: [30/782]\tTime 0.137 (0.707)\tLoss 77.8733 (82.1692)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.504)\n",
      "Test: [40/782]\tTime 0.138 (0.568)\tLoss 81.0293 (83.3114)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.457)\n",
      "Test: [50/782]\tTime 0.128 (0.551)\tLoss 90.7543 (83.6301)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.490)\n",
      "Test: [60/782]\tTime 0.122 (0.517)\tLoss 80.4857 (83.8946)\tPrec@1 0.000 (0.051)\tPrec@5 0.000 (0.512)\n",
      "Test: [70/782]\tTime 0.142 (0.462)\tLoss 72.9529 (83.4917)\tPrec@1 0.000 (0.044)\tPrec@5 0.000 (0.440)\n",
      "Test: [80/782]\tTime 0.139 (0.430)\tLoss 79.7847 (83.1494)\tPrec@1 0.000 (0.039)\tPrec@5 0.000 (0.656)\n",
      "Test: [90/782]\tTime 0.307 (0.404)\tLoss 71.6949 (82.9933)\tPrec@1 0.000 (0.034)\tPrec@5 0.000 (0.584)\n",
      "Test: [100/782]\tTime 0.122 (0.382)\tLoss 80.3646 (82.3276)\tPrec@1 0.000 (0.186)\tPrec@5 0.000 (0.681)\n",
      "Test: [110/782]\tTime 0.211 (0.363)\tLoss 78.5619 (82.8626)\tPrec@1 0.000 (0.169)\tPrec@5 0.000 (0.619)\n",
      "Test: [120/782]\tTime 0.122 (0.348)\tLoss 76.0554 (82.3196)\tPrec@1 0.000 (0.155)\tPrec@5 0.000 (0.659)\n",
      "Test: [130/782]\tTime 0.122 (0.335)\tLoss 93.2063 (82.3606)\tPrec@1 0.000 (0.143)\tPrec@5 0.000 (0.608)\n",
      "Test: [140/782]\tTime 0.122 (0.330)\tLoss 91.6535 (82.1925)\tPrec@1 0.000 (0.133)\tPrec@5 1.562 (0.598)\n",
      "Test: [150/782]\tTime 0.120 (0.325)\tLoss 82.2019 (82.3580)\tPrec@1 0.000 (0.135)\tPrec@5 1.562 (0.621)\n",
      "Test: [160/782]\tTime 0.119 (0.318)\tLoss 96.6406 (82.4602)\tPrec@1 0.000 (0.126)\tPrec@5 0.000 (0.621)\n",
      "Test: [170/782]\tTime 0.113 (0.318)\tLoss 81.4928 (82.5937)\tPrec@1 0.000 (0.119)\tPrec@5 1.562 (0.640)\n",
      "Test: [180/782]\tTime 0.110 (0.318)\tLoss 78.8753 (82.5844)\tPrec@1 0.000 (0.112)\tPrec@5 1.562 (0.665)\n",
      "Test: [190/782]\tTime 1.348 (0.325)\tLoss 83.0733 (82.6384)\tPrec@1 0.000 (0.106)\tPrec@5 3.125 (0.679)\n",
      "Test: [200/782]\tTime 1.428 (0.325)\tLoss 77.2349 (82.5995)\tPrec@1 0.000 (0.101)\tPrec@5 3.125 (0.676)\n",
      "Test: [210/782]\tTime 0.096 (0.315)\tLoss 88.9150 (82.6875)\tPrec@1 0.000 (0.104)\tPrec@5 0.000 (0.659)\n",
      "Test: [220/782]\tTime 0.096 (0.308)\tLoss 76.5506 (82.7705)\tPrec@1 0.000 (0.099)\tPrec@5 1.562 (0.650)\n",
      "Test: [230/782]\tTime 0.096 (0.303)\tLoss 78.5494 (83.2749)\tPrec@1 0.000 (0.095)\tPrec@5 6.250 (0.663)\n",
      "Test: [240/782]\tTime 0.096 (0.298)\tLoss 79.0711 (83.3818)\tPrec@1 0.000 (0.091)\tPrec@5 0.000 (0.674)\n",
      "Test: [250/782]\tTime 0.101 (0.298)\tLoss 80.7647 (83.4131)\tPrec@1 0.000 (0.087)\tPrec@5 0.000 (0.685)\n",
      "Test: [260/782]\tTime 0.111 (0.299)\tLoss 85.6965 (83.3179)\tPrec@1 0.000 (0.084)\tPrec@5 1.562 (0.700)\n",
      "Test: [270/782]\tTime 0.103 (0.295)\tLoss 87.8112 (83.3405)\tPrec@1 0.000 (0.081)\tPrec@5 1.562 (0.726)\n",
      "Test: [280/782]\tTime 0.117 (0.293)\tLoss 84.7276 (83.3291)\tPrec@1 0.000 (0.078)\tPrec@5 0.000 (0.712)\n",
      "Test: [290/782]\tTime 0.113 (0.294)\tLoss 84.5340 (83.3146)\tPrec@1 0.000 (0.075)\tPrec@5 0.000 (0.703)\n",
      "Test: [300/782]\tTime 0.114 (0.291)\tLoss 82.5587 (83.3596)\tPrec@1 0.000 (0.073)\tPrec@5 0.000 (0.711)\n",
      "Test: [310/782]\tTime 0.123 (0.290)\tLoss 76.1877 (83.3597)\tPrec@1 0.000 (0.070)\tPrec@5 0.000 (0.688)\n",
      "Test: [320/782]\tTime 0.118 (0.285)\tLoss 79.7643 (83.2633)\tPrec@1 0.000 (0.068)\tPrec@5 0.000 (0.681)\n",
      "Test: [330/782]\tTime 0.142 (0.281)\tLoss 81.2491 (83.2432)\tPrec@1 0.000 (0.066)\tPrec@5 1.562 (0.666)\n",
      "Test: [340/782]\tTime 0.120 (0.280)\tLoss 90.3090 (83.2747)\tPrec@1 0.000 (0.069)\tPrec@5 0.000 (0.660)\n",
      "Test: [350/782]\tTime 0.122 (0.280)\tLoss 91.6359 (83.3505)\tPrec@1 0.000 (0.071)\tPrec@5 0.000 (0.650)\n",
      "Test: [360/782]\tTime 0.333 (0.277)\tLoss 79.1180 (83.3185)\tPrec@1 0.000 (0.069)\tPrec@5 0.000 (0.645)\n",
      "Test: [370/782]\tTime 0.143 (0.274)\tLoss 88.8606 (83.2621)\tPrec@1 0.000 (0.067)\tPrec@5 0.000 (0.628)\n",
      "Test: [380/782]\tTime 0.123 (0.272)\tLoss 73.8847 (83.2882)\tPrec@1 0.000 (0.066)\tPrec@5 0.000 (0.611)\n",
      "Test: [390/782]\tTime 0.122 (0.269)\tLoss 86.6933 (83.3552)\tPrec@1 0.000 (0.064)\tPrec@5 0.000 (0.595)\n",
      "Test: [400/782]\tTime 0.236 (0.267)\tLoss 74.5866 (83.1860)\tPrec@1 0.000 (0.062)\tPrec@5 0.000 (0.581)\n",
      "Test: [410/782]\tTime 0.122 (0.265)\tLoss 87.5213 (83.1284)\tPrec@1 0.000 (0.061)\tPrec@5 0.000 (0.566)\n",
      "Test: [420/782]\tTime 0.346 (0.264)\tLoss 75.2917 (82.9935)\tPrec@1 0.000 (0.059)\tPrec@5 0.000 (0.553)\n",
      "Test: [430/782]\tTime 0.122 (0.262)\tLoss 92.4512 (82.8240)\tPrec@1 0.000 (0.058)\tPrec@5 0.000 (0.540)\n",
      "Test: [440/782]\tTime 0.343 (0.261)\tLoss 83.9227 (82.8010)\tPrec@1 0.000 (0.057)\tPrec@5 0.000 (0.528)\n",
      "Test: [450/782]\tTime 0.122 (0.259)\tLoss 86.5243 (82.7520)\tPrec@1 0.000 (0.055)\tPrec@5 0.000 (0.516)\n",
      "Test: [460/782]\tTime 0.231 (0.258)\tLoss 89.1939 (82.7066)\tPrec@1 0.000 (0.054)\tPrec@5 0.000 (0.505)\n",
      "Test: [470/782]\tTime 0.122 (0.256)\tLoss 79.1785 (82.7085)\tPrec@1 0.000 (0.053)\tPrec@5 0.000 (0.557)\n",
      "Test: [480/782]\tTime 0.335 (0.256)\tLoss 99.2431 (82.9678)\tPrec@1 0.000 (0.052)\tPrec@5 0.000 (0.546)\n",
      "Test: [490/782]\tTime 0.122 (0.255)\tLoss 68.2826 (82.8992)\tPrec@1 0.000 (0.051)\tPrec@5 0.000 (0.535)\n",
      "Test: [500/782]\tTime 0.464 (0.253)\tLoss 77.4983 (82.7266)\tPrec@1 0.000 (0.050)\tPrec@5 0.000 (0.568)\n",
      "Test: [510/782]\tTime 0.122 (0.252)\tLoss 76.9936 (82.6802)\tPrec@1 0.000 (0.049)\tPrec@5 0.000 (0.557)\n",
      "Test: [520/782]\tTime 0.565 (0.252)\tLoss 62.1808 (82.5771)\tPrec@1 0.000 (0.147)\tPrec@5 0.000 (0.645)\n",
      "Test: [530/782]\tTime 0.122 (0.250)\tLoss 78.5431 (82.6249)\tPrec@1 0.000 (0.144)\tPrec@5 0.000 (0.633)\n",
      "Test: [540/782]\tTime 0.321 (0.249)\tLoss 96.1035 (82.7168)\tPrec@1 0.000 (0.142)\tPrec@5 0.000 (0.621)\n",
      "Test: [550/782]\tTime 0.123 (0.247)\tLoss 92.3054 (82.7448)\tPrec@1 0.000 (0.139)\tPrec@5 0.000 (0.610)\n",
      "Test: [560/782]\tTime 0.344 (0.246)\tLoss 81.4174 (82.7264)\tPrec@1 0.000 (0.136)\tPrec@5 0.000 (0.610)\n",
      "Test: [570/782]\tTime 0.123 (0.245)\tLoss 91.2975 (82.6416)\tPrec@1 0.000 (0.134)\tPrec@5 0.000 (0.613)\n",
      "Test: [580/782]\tTime 0.374 (0.244)\tLoss 100.5390 (82.6747)\tPrec@1 0.000 (0.132)\tPrec@5 0.000 (0.602)\n",
      "Test: [590/782]\tTime 0.123 (0.243)\tLoss 71.5049 (82.6582)\tPrec@1 0.000 (0.130)\tPrec@5 0.000 (0.592)\n",
      "Test: [600/782]\tTime 0.319 (0.242)\tLoss 57.0313 (82.6304)\tPrec@1 0.000 (0.127)\tPrec@5 0.000 (0.582)\n",
      "Test: [610/782]\tTime 0.122 (0.241)\tLoss 85.2778 (82.6336)\tPrec@1 0.000 (0.125)\tPrec@5 0.000 (0.583)\n",
      "Test: [620/782]\tTime 0.286 (0.240)\tLoss 82.3146 (82.5646)\tPrec@1 0.000 (0.123)\tPrec@5 0.000 (0.574)\n",
      "Test: [630/782]\tTime 0.126 (0.240)\tLoss 82.7413 (82.5001)\tPrec@1 0.000 (0.121)\tPrec@5 3.125 (0.572)\n",
      "Test: [640/782]\tTime 0.138 (0.241)\tLoss 83.8653 (82.5296)\tPrec@1 1.562 (0.122)\tPrec@5 1.562 (0.568)\n",
      "Test: [650/782]\tTime 0.132 (0.240)\tLoss 75.9595 (82.5375)\tPrec@1 0.000 (0.120)\tPrec@5 0.000 (0.562)\n",
      "Test: [660/782]\tTime 0.133 (0.241)\tLoss 88.5352 (82.5777)\tPrec@1 0.000 (0.118)\tPrec@5 0.000 (0.556)\n",
      "Test: [670/782]\tTime 0.136 (0.240)\tLoss 81.8406 (82.6315)\tPrec@1 0.000 (0.116)\tPrec@5 0.000 (0.547)\n",
      "Test: [680/782]\tTime 0.141 (0.241)\tLoss 76.7394 (82.6470)\tPrec@1 0.000 (0.115)\tPrec@5 0.000 (0.541)\n",
      "Test: [690/782]\tTime 0.139 (0.240)\tLoss 89.3199 (82.6417)\tPrec@1 0.000 (0.113)\tPrec@5 0.000 (0.536)\n",
      "Test: [700/782]\tTime 0.139 (0.239)\tLoss 92.0236 (82.6810)\tPrec@1 0.000 (0.111)\tPrec@5 0.000 (0.528)\n",
      "Test: [710/782]\tTime 0.141 (0.240)\tLoss 95.5357 (82.6597)\tPrec@1 0.000 (0.110)\tPrec@5 0.000 (0.521)\n",
      "Test: [720/782]\tTime 0.141 (0.238)\tLoss 81.8158 (82.5850)\tPrec@1 0.000 (0.108)\tPrec@5 0.000 (0.514)\n",
      "Test: [730/782]\tTime 0.143 (0.237)\tLoss 95.5706 (82.5794)\tPrec@1 0.000 (0.107)\tPrec@5 0.000 (0.515)\n",
      "Test: [740/782]\tTime 0.170 (0.236)\tLoss 99.7104 (82.6074)\tPrec@1 0.000 (0.105)\tPrec@5 0.000 (0.508)\n",
      "Test: [750/782]\tTime 0.141 (0.235)\tLoss 88.7791 (82.6248)\tPrec@1 0.000 (0.104)\tPrec@5 0.000 (0.503)\n",
      "Test: [760/782]\tTime 0.171 (0.234)\tLoss 75.5750 (82.5556)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.501)\n",
      "Test: [770/782]\tTime 1.098 (0.240)\tLoss 83.7222 (82.5546)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.497)\n",
      "Test: [780/782]\tTime 4.264 (0.267)\tLoss 80.3824 (82.5817)\tPrec@1 0.000 (0.100)\tPrec@5 0.000 (0.492)\n",
      "~~3\t0.06587002416666667\t0.492\n",
      "\n",
      " * Prec@1 0.100 Prec@5 0.492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.set_epoch(int(args.epochs*0.92+0.5)+args.warmup)\n",
    "validate(dm.val_dl, len(dm.val_dl), model, criterion, 3, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1562.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.20703125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1562.5/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390.625"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
