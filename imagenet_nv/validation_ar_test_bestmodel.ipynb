{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation by aspect ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of center cropping, sort validation images by aspect ratio. Crop batches of these images based on the closest aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, shutil, time, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import models\n",
    "from fp16util import network_to_half, set_grad, copy_in_params\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n",
    "#print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "    parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
    "    parser.add_argument('--save-dir', type=str, default=Path.cwd(), help='Directory to save logs and models.')\n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet50')\n",
    "    # parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet18',\n",
    "    #                     choices=model_names,\n",
    "    #                     help='model architecture: ' +\n",
    "    #                     ' | '.join(model_names) +\n",
    "    #                     ' (default: resnet18)')\n",
    "    parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--epochs', default=45, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('--warmup', default=0, type=int, metavar='N',\n",
    "                        help='number of additional epochs to warmup')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 256)')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.4, type=float,\n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                        metavar='N', help='print frequency (default: 10)')\n",
    "    parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--small', action='store_true', help='start with smaller images')\n",
    "    parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                        help='evaluate model on validation set')\n",
    "    parser.add_argument('--pretrained', dest='pretrained', action='store_true', help='use pre-trained model')\n",
    "    parser.add_argument('--fp16', action='store_true', help='Run model fp16 mode.')\n",
    "    parser.add_argument('--dp', action='store_true', help='Run model fp16 mode.')\n",
    "    parser.add_argument('--sz',       default=224, type=int, help='Size of transformed image.')\n",
    "    parser.add_argument('--decay-int', default=30, type=int, help='Decay LR by 10 every decay-int epochs')\n",
    "    parser.add_argument('--loss-scale', type=float, default=1,\n",
    "                        help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "    parser.add_argument('--prof', dest='prof', action='store_true', help='Only run a few iters for profiling.')\n",
    "\n",
    "    parser.add_argument('--distributed', action='store_true', help='Run distributed training')\n",
    "    parser.add_argument('--dist-url', default='file://sync.file', type=str,\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=0, type=int,\n",
    "                        help='Used for multi-process training. Can either be manually set ' +\n",
    "                        'or automatically set by using \\'python -m multiproc\\'.')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_path = str(Path.home()/'7-8x_train_lr3d2_e68_b128_93_success/model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "args = get_parser().parse_args(['/home/paperspace/data/imagenet', '--evaluate', '--resume', resume_path])\n",
    "if args.local_rank > 0: sys.stdout = open(f'{args.save_dir}/GPU_{args.local_rank}.log', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def sort_ar(valdir):\n",
    "    if os.path.isfile('sorted_idxar.p'): return pickle.load(open('sorted_idxar.p', 'rb'))\n",
    "    val_dataset = datasets.ImageFolder(valdir)\n",
    "    sizes = [img[0].size for img in val_dataset]\n",
    "    idx_ar = [(i, round(s[0]/s[1], 5)) for i,s in enumerate(sizes)]\n",
    "    sorted_idxar = sorted(idx_ar, key=lambda x: x[1])\n",
    "    pickle.dump(sorted_idxar, open('sorted_idxar.p', 'wb'))\n",
    "    return sorted_idxar\n",
    "\n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "\n",
    "def map_idx2ar(idx_ar_sorted, batch_size):\n",
    "    ar_chunks = list(chunks(idx_ar_sorted, batch_size))\n",
    "    idx2ar = {}\n",
    "    for chunk in ar_chunks:\n",
    "        idxs, ars = list(zip(*chunk))\n",
    "        mean = round(np.mean(ars), 5)\n",
    "        for idx in idxs:\n",
    "            idx2ar[idx] = mean\n",
    "    return idx2ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None, ar_transform=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "        self.ar_transform = ar_transform\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.ar_transform(sample, index)\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "class ARSampler(Sampler):\n",
    "    def __init__(self, indices): self.indices = indices\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __iter__(self): return iter(self.indices)\n",
    "    \n",
    "\n",
    "class CropArTfm(object):\n",
    "    def __init__(self, idx2ar, target_size):\n",
    "        self.idx2ar, self.target_size = idx2ar, target_size\n",
    "    def __call__(self, img, idx):\n",
    "        target_ar = self.idx2ar[idx]\n",
    "        if target_ar < 1: \n",
    "            w = int(self.target_size/target_ar)\n",
    "            size = (w//8*8, self.target_size)\n",
    "        else: \n",
    "            h = int(self.target_size*target_ar)\n",
    "            size = (self.target_size, h//8*8)\n",
    "        return torchvision.transforms.functional.center_crop(img, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(valdir, batch_size, target_size, tensor_tfm, use_val_sampler, use_ar_sampler):\n",
    "    idx_ar_sorted = sort_ar(valdir)\n",
    "    idx_sorted, _ = zip(*idx_ar_sorted)\n",
    "    idx2ar = map_idx2ar(idx_ar_sorted, batch_size)\n",
    "    \n",
    "    if use_ar_sampler:\n",
    "        val_dataset = ValDataset(valdir, transforms.Compose(tensor_tfm), ar_transform=CropArTfm(idx2ar, target_size))\n",
    "        val_sampler = ARSampler(idx_sorted)\n",
    "        return val_dataset, val_sampler\n",
    "    \n",
    "    val_tfms = [transforms.Resize(int(args.sz*1.14)), transforms.CenterCrop(args.sz)] + tensor_tfm\n",
    "    val_dataset = datasets.ImageFolder(valdir,  transforms.Compose(val_tfms))\n",
    "    val_sampler = None\n",
    "    if use_val_sampler and args.distributed:\n",
    "        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
    "    return val_dataset, val_sampler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(traindir, valdir, bs, sz, val_bs=None, use_val_sampler=True, use_ar_sampler=False, min_scale=0.08):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    tensor_tfm = [transforms.ToTensor(), normalize]\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        traindir, transforms.Compose([\n",
    "            transforms.RandomResizedCrop(sz, scale=(min_scale, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ] + tensor_tfm))\n",
    "    train_sampler = (torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "    val_bs = val_bs or bs\n",
    "    val_dataset, val_sampler = create_validation_set(valdir, val_bs, sz, tensor_tfm, use_val_sampler, use_ar_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True, sampler=val_sampler)\n",
    "\n",
    "    return train_loader,val_loader,train_sampler,val_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item() is a recent addition, so this helps with backward compatibility.\n",
    "def to_python_float(t):\n",
    "    if hasattr(t, 'item'):\n",
    "        return t.item()\n",
    "    else:\n",
    "        return t[0]\n",
    "\n",
    "class data_prefetcher():\n",
    "    def __init__(self, loader, prefetch=True):\n",
    "        self.loader,self.prefetch = iter(loader),prefetch\n",
    "        if prefetch:\n",
    "            self.stream = torch.cuda.Stream()\n",
    "            self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def next(self):\n",
    "        if not self.prefetch:\n",
    "            input,target = next(self.loader)\n",
    "            return input.cuda(async=True),target.cuda(async=True)\n",
    "\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        input = self.next_input\n",
    "        target = self.next_target\n",
    "        self.preload()\n",
    "        return input, target\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch, start_time):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    prefetcher = data_prefetcher(val_loader)\n",
    "    input, target = prefetcher.next()\n",
    "    i = -1\n",
    "    while input is not None:\n",
    "        i += 1\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = Variable(input)\n",
    "        target_var = Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "        if args.distributed:\n",
    "            reduced_loss = reduce_tensor(loss.data)\n",
    "            prec1 = reduce_tensor(prec1)\n",
    "            prec5 = reduce_tensor(prec5)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "            \n",
    "\n",
    "        losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "        top1.update(to_python_float(prec1), input.size(0))\n",
    "        top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if args.local_rank == 0 and i % args.print_freq == 0:\n",
    "            output = ('Test: [{0}/{1}]\\t' \\\n",
    "                    + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                    + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                    + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n",
    "                    + 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})').format(\n",
    "                    i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    top1=top1, top5=top5)\n",
    "            print(output)\n",
    "            with open(f'{args.save_dir}/full.log', 'a') as f:\n",
    "                f.write(output + '\\n')\n",
    "\n",
    "        input, target = prefetcher.next()\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    print(f'~~{epoch}\\t{float(time_diff.total_seconds() / 3600.0)}\\t{top5.avg:.3f}\\n')\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, f'{args.save_dir}/model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    size = dist.get_world_size()\n",
    "    # rt /= args.world_size\n",
    "    rt /= size\n",
    "    return rt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.fp16 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~epoch\thours\ttop1Accuracy\n",
      "\n",
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "print(\"~~epoch\\thours\\ttop1Accuracy\\n\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "if args.distributed:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url)\n",
    "    print('Distributed: init_process_group success')\n",
    "\n",
    "if args.fp16: assert torch.backends.cudnn.enabled, \"fp16 mode requires cudnn backend to be enabled.\"\n",
    "\n",
    "# create model\n",
    "#     if args.pretrained: model = models.__dict__[args.arch](pretrained=True)\n",
    "#     else: model = models.__dict__[args.arch]()\n",
    "# AS: force use resnet50 for now, until we figure out whether to upload model directory\n",
    "import resnet\n",
    "model = resnet.resnet50()\n",
    "\n",
    "print(\"Loaded model\")\n",
    "\n",
    "model = model.cuda()\n",
    "n_dev = torch.cuda.device_count()\n",
    "if args.fp16: model = network_to_half(model)\n",
    "if args.distributed: model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "elif args.dp:\n",
    "    model = nn.DataParallel(model)\n",
    "    args.batch_size *= n_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined loss and optimizer\n"
     ]
    }
   ],
   "source": [
    "global param_copy\n",
    "if args.fp16:\n",
    "    param_copy = [param.clone().type(torch.cuda.FloatTensor).detach() for param in model.parameters()]\n",
    "    for param in param_copy: param.requires_grad = True\n",
    "else: param_copy = list(model.parameters())\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(param_copy, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "print(\"Defined loss and optimizer\")\n",
    "\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWrap(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    def forward(self, x):\n",
    "        return self.module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to wrap inside module - sinde checkpoint had a Distributed wrapper around it\n",
    "model = DWrap(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally resume from a checkpoint\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(0))\n",
    "#             checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(args.gpu))\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    else: print(\"=> no checkpoint found at '{}'\".format(args.resume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_wrapper(fn):\n",
    "    return lambda x,idx: fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None, ar_transform=None, resize=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "        self.ar_transform = ar_transform\n",
    "        self.resize = resize\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            if self.resize: sample = self.resize(sample)\n",
    "            sample = self.ar_transform(sample, index)\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "valdir = os.path.join(args.data, 'validation')\n",
    "val_bs = 128\n",
    "target_size = 288\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "tensor_tfm = [transforms.ToTensor(), normalize]\n",
    "\n",
    "idx_ar_sorted = sort_ar(valdir)\n",
    "idx_sorted, _ = zip(*idx_ar_sorted)\n",
    "idx2ar = map_idx2ar(idx_ar_sorted, val_bs)\n",
    "\n",
    "val_dataset_ar = ValDataset(valdir, transforms.Compose(tensor_tfm), ar_transform=CropArTfm(idx2ar, target_size))\n",
    "val_sampler_ar = ARSampler(idx_sorted)\n",
    "\n",
    "val_dataset_ar_rs = ValDataset(valdir, transforms.Compose(tensor_tfm), ar_transform=CropArTfm(idx2ar, target_size), resize=transforms.Resize(int(target_size*1.14)))\n",
    "\n",
    "val_tfms = [transforms.Resize(int(target_size*1.14)), transforms.CenterCrop(target_size)] + tensor_tfm\n",
    "val_dataset = datasets.ImageFolder(valdir,  transforms.Compose(val_tfms))\n",
    "# val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/391]\tTime 5.829 (5.829)\tLoss 1.5127 (1.5127)\tPrec@1 66.406 (66.406)\tPrec@5 87.500 (87.500)\n",
      "Test: [10/391]\tTime 0.128 (1.434)\tLoss 1.0293 (1.0241)\tPrec@1 79.688 (74.077)\tPrec@5 92.969 (92.614)\n",
      "Test: [20/391]\tTime 2.486 (0.962)\tLoss 1.0449 (1.0817)\tPrec@1 71.094 (72.693)\tPrec@5 92.188 (92.225)\n",
      "Test: [30/391]\tTime 0.120 (0.978)\tLoss 0.8711 (1.0730)\tPrec@1 80.469 (73.236)\tPrec@5 92.969 (91.935)\n",
      "Test: [40/391]\tTime 0.148 (0.778)\tLoss 1.3506 (1.0414)\tPrec@1 64.062 (73.990)\tPrec@5 89.062 (92.226)\n",
      "Test: [50/391]\tTime 0.263 (0.680)\tLoss 1.2803 (1.0966)\tPrec@1 72.656 (73.146)\tPrec@5 88.281 (91.376)\n",
      "Test: [60/391]\tTime 0.434 (0.621)\tLoss 1.6611 (1.1348)\tPrec@1 57.031 (72.080)\tPrec@5 85.156 (90.868)\n",
      "Test: [70/391]\tTime 0.113 (0.570)\tLoss 1.1621 (1.1244)\tPrec@1 72.656 (72.348)\tPrec@5 91.406 (91.032)\n",
      "Test: [80/391]\tTime 2.372 (0.593)\tLoss 1.0908 (1.1000)\tPrec@1 72.656 (72.820)\tPrec@5 92.188 (91.310)\n",
      "Test: [90/391]\tTime 0.101 (0.593)\tLoss 1.3398 (1.0952)\tPrec@1 67.188 (72.879)\tPrec@5 90.625 (91.380)\n",
      "Test: [100/391]\tTime 2.335 (0.637)\tLoss 1.1787 (1.0932)\tPrec@1 70.312 (72.888)\tPrec@5 90.625 (91.414)\n",
      "Test: [110/391]\tTime 0.124 (0.589)\tLoss 1.7539 (1.0995)\tPrec@1 58.594 (72.755)\tPrec@5 82.031 (91.371)\n",
      "Test: [120/391]\tTime 0.130 (0.561)\tLoss 1.5879 (1.1419)\tPrec@1 64.844 (71.849)\tPrec@5 84.375 (90.786)\n",
      "Test: [130/391]\tTime 2.008 (0.572)\tLoss 1.2480 (1.1397)\tPrec@1 65.625 (71.935)\tPrec@5 87.500 (90.720)\n",
      "Test: [140/391]\tTime 0.103 (0.566)\tLoss 0.9453 (1.1265)\tPrec@1 77.344 (72.219)\tPrec@5 90.625 (90.847)\n",
      "Test: [150/391]\tTime 0.110 (0.569)\tLoss 0.6431 (1.1166)\tPrec@1 83.594 (72.449)\tPrec@5 94.531 (90.894)\n",
      "Test: [160/391]\tTime 0.109 (0.553)\tLoss 1.1221 (1.0994)\tPrec@1 68.750 (72.879)\tPrec@5 92.969 (91.086)\n",
      "Test: [170/391]\tTime 0.110 (0.538)\tLoss 0.8008 (1.0903)\tPrec@1 78.125 (73.099)\tPrec@5 94.531 (91.182)\n",
      "Test: [180/391]\tTime 0.111 (0.525)\tLoss 0.4360 (1.0811)\tPrec@1 89.062 (73.256)\tPrec@5 99.219 (91.272)\n",
      "Test: [190/391]\tTime 0.133 (0.511)\tLoss 0.6284 (1.0700)\tPrec@1 85.156 (73.462)\tPrec@5 93.750 (91.464)\n",
      "Test: [200/391]\tTime 0.757 (0.501)\tLoss 1.2881 (1.0531)\tPrec@1 66.406 (73.850)\tPrec@5 91.406 (91.655)\n",
      "Test: [210/391]\tTime 0.112 (0.488)\tLoss 0.8555 (1.0431)\tPrec@1 78.125 (74.071)\tPrec@5 95.312 (91.832)\n",
      "Test: [220/391]\tTime 0.909 (0.483)\tLoss 0.5264 (1.0338)\tPrec@1 89.062 (74.208)\tPrec@5 95.312 (91.979)\n",
      "Test: [230/391]\tTime 0.121 (0.472)\tLoss 0.5527 (1.0266)\tPrec@1 87.500 (74.418)\tPrec@5 96.094 (92.056)\n",
      "Test: [240/391]\tTime 0.621 (0.466)\tLoss 1.4688 (1.0385)\tPrec@1 60.156 (74.157)\tPrec@5 89.062 (91.925)\n",
      "Test: [250/391]\tTime 0.112 (0.457)\tLoss 1.0059 (1.0464)\tPrec@1 73.438 (73.976)\tPrec@5 90.625 (91.814)\n",
      "Test: [260/391]\tTime 0.767 (0.451)\tLoss 1.6133 (1.0527)\tPrec@1 63.281 (73.851)\tPrec@5 84.375 (91.700)\n",
      "Test: [270/391]\tTime 0.112 (0.444)\tLoss 1.0322 (1.0572)\tPrec@1 74.219 (73.757)\tPrec@5 90.625 (91.643)\n",
      "Test: [280/391]\tTime 0.936 (0.439)\tLoss 1.1885 (1.0607)\tPrec@1 69.531 (73.688)\tPrec@5 88.281 (91.551)\n",
      "Test: [290/391]\tTime 0.129 (0.433)\tLoss 1.5293 (1.0675)\tPrec@1 57.812 (73.510)\tPrec@5 87.500 (91.471)\n",
      "Test: [300/391]\tTime 0.726 (0.428)\tLoss 0.9414 (1.0700)\tPrec@1 74.219 (73.399)\tPrec@5 91.406 (91.448)\n",
      "Test: [310/391]\tTime 0.112 (0.423)\tLoss 1.6201 (1.0661)\tPrec@1 55.469 (73.465)\tPrec@5 85.156 (91.497)\n",
      "Test: [320/391]\tTime 2.382 (0.429)\tLoss 1.0811 (1.0640)\tPrec@1 71.875 (73.493)\tPrec@5 92.188 (91.521)\n",
      "Test: [330/391]\tTime 0.129 (0.431)\tLoss 0.9121 (1.0597)\tPrec@1 74.219 (73.600)\tPrec@5 96.094 (91.595)\n",
      "Test: [340/391]\tTime 0.127 (0.435)\tLoss 0.8989 (1.0569)\tPrec@1 80.469 (73.669)\tPrec@5 92.969 (91.631)\n",
      "Test: [350/391]\tTime 0.266 (0.429)\tLoss 0.6880 (1.0522)\tPrec@1 85.156 (73.762)\tPrec@5 95.312 (91.709)\n",
      "Test: [360/391]\tTime 0.129 (0.426)\tLoss 0.6689 (1.0473)\tPrec@1 85.938 (73.860)\tPrec@5 95.312 (91.800)\n",
      "Test: [370/391]\tTime 0.129 (0.420)\tLoss 1.0957 (1.0443)\tPrec@1 75.781 (73.966)\tPrec@5 91.406 (91.853)\n",
      "Test: [380/391]\tTime 0.351 (0.417)\tLoss 1.0078 (1.0430)\tPrec@1 71.094 (74.005)\tPrec@5 92.969 (91.876)\n",
      "Test: [390/391]\tTime 3.748 (0.471)\tLoss 1.5479 (1.0472)\tPrec@1 76.250 (73.950)\tPrec@5 90.000 (91.848)\n",
      "~~0\t0.12323614166666666\t91.848\n",
      "\n",
      " * Prec@1 73.950 Prec@5 91.848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.95"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "validate(val_loader, model, criterion, 0, start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test original with AR sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/391]\tTime 1.912 (1.912)\tLoss 1.3594 (1.3594)\tPrec@1 67.188 (67.188)\tPrec@5 86.719 (86.719)\n",
      "Test: [10/391]\tTime 0.088 (0.400)\tLoss 1.0459 (0.9636)\tPrec@1 79.688 (75.000)\tPrec@5 89.062 (92.259)\n",
      "Test: [20/391]\tTime 0.088 (0.392)\tLoss 0.9121 (1.0274)\tPrec@1 75.000 (73.772)\tPrec@5 95.312 (91.592)\n",
      "Test: [30/391]\tTime 0.088 (0.350)\tLoss 0.8262 (1.0025)\tPrec@1 82.031 (74.320)\tPrec@5 93.750 (92.087)\n",
      "Test: [40/391]\tTime 0.088 (0.357)\tLoss 1.0703 (0.9653)\tPrec@1 71.094 (75.305)\tPrec@5 92.969 (92.530)\n",
      "Test: [50/391]\tTime 0.090 (0.337)\tLoss 1.2402 (1.0169)\tPrec@1 69.531 (74.357)\tPrec@5 92.969 (91.881)\n",
      "Test: [60/391]\tTime 0.088 (0.340)\tLoss 1.7568 (1.0623)\tPrec@1 54.688 (73.335)\tPrec@5 83.594 (91.304)\n",
      "Test: [70/391]\tTime 0.088 (0.331)\tLoss 1.1191 (1.0536)\tPrec@1 74.219 (73.537)\tPrec@5 89.844 (91.384)\n",
      "Test: [80/391]\tTime 0.088 (0.331)\tLoss 0.9688 (1.0258)\tPrec@1 75.000 (74.199)\tPrec@5 90.625 (91.763)\n",
      "Test: [90/391]\tTime 0.088 (0.324)\tLoss 1.0059 (1.0122)\tPrec@1 73.438 (74.511)\tPrec@5 93.750 (92.033)\n",
      "Test: [100/391]\tTime 0.088 (0.328)\tLoss 0.8213 (0.9909)\tPrec@1 78.125 (74.954)\tPrec@5 94.531 (92.358)\n",
      "Test: [110/391]\tTime 0.094 (0.322)\tLoss 1.1328 (0.9812)\tPrec@1 71.875 (75.197)\tPrec@5 89.844 (92.483)\n",
      "Test: [120/391]\tTime 0.088 (0.325)\tLoss 1.2432 (0.9991)\tPrec@1 71.875 (74.858)\tPrec@5 89.062 (92.246)\n",
      "Test: [130/391]\tTime 0.100 (0.321)\tLoss 0.8037 (0.9871)\tPrec@1 78.906 (75.066)\tPrec@5 92.188 (92.343)\n",
      "Test: [140/391]\tTime 0.088 (0.325)\tLoss 0.8145 (0.9729)\tPrec@1 79.688 (75.366)\tPrec@5 95.312 (92.537)\n",
      "Test: [150/391]\tTime 0.088 (0.321)\tLoss 0.5723 (0.9639)\tPrec@1 84.375 (75.590)\tPrec@5 96.094 (92.627)\n",
      "Test: [160/391]\tTime 0.088 (0.322)\tLoss 1.0430 (0.9493)\tPrec@1 72.656 (76.009)\tPrec@5 92.188 (92.770)\n",
      "Test: [170/391]\tTime 0.088 (0.318)\tLoss 0.5981 (0.9418)\tPrec@1 81.250 (76.183)\tPrec@5 96.875 (92.845)\n",
      "Test: [180/391]\tTime 0.088 (0.322)\tLoss 0.4099 (0.9340)\tPrec@1 87.500 (76.308)\tPrec@5 99.219 (92.908)\n",
      "Test: [190/391]\tTime 0.101 (0.318)\tLoss 0.5298 (0.9302)\tPrec@1 87.500 (76.383)\tPrec@5 95.312 (92.993)\n",
      "Test: [200/391]\tTime 0.088 (0.319)\tLoss 1.1523 (0.9175)\tPrec@1 64.844 (76.636)\tPrec@5 95.312 (93.151)\n",
      "Test: [210/391]\tTime 0.088 (0.316)\tLoss 0.8389 (0.9106)\tPrec@1 71.875 (76.759)\tPrec@5 93.750 (93.280)\n",
      "Test: [220/391]\tTime 0.088 (0.318)\tLoss 0.4783 (0.9036)\tPrec@1 87.500 (76.796)\tPrec@5 96.094 (93.425)\n",
      "Test: [230/391]\tTime 0.088 (0.316)\tLoss 0.5186 (0.8972)\tPrec@1 89.062 (76.972)\tPrec@5 97.656 (93.493)\n",
      "Test: [240/391]\tTime 0.130 (0.318)\tLoss 1.3730 (0.9086)\tPrec@1 61.719 (76.699)\tPrec@5 88.281 (93.355)\n",
      "Test: [250/391]\tTime 0.311 (0.316)\tLoss 0.9868 (0.9176)\tPrec@1 75.000 (76.528)\tPrec@5 89.844 (93.258)\n",
      "Test: [260/391]\tTime 0.288 (0.317)\tLoss 1.3574 (0.9259)\tPrec@1 66.406 (76.404)\tPrec@5 85.938 (93.130)\n",
      "Test: [270/391]\tTime 0.537 (0.317)\tLoss 1.1650 (0.9322)\tPrec@1 72.656 (76.283)\tPrec@5 89.844 (93.067)\n",
      "Test: [280/391]\tTime 0.787 (0.318)\tLoss 1.1777 (0.9386)\tPrec@1 74.219 (76.184)\tPrec@5 87.500 (92.927)\n",
      "Test: [290/391]\tTime 0.151 (0.317)\tLoss 1.3057 (0.9470)\tPrec@1 63.281 (76.007)\tPrec@5 89.062 (92.832)\n",
      "Test: [300/391]\tTime 0.594 (0.317)\tLoss 0.9692 (0.9517)\tPrec@1 71.094 (75.836)\tPrec@5 93.750 (92.774)\n",
      "Test: [310/391]\tTime 0.231 (0.317)\tLoss 1.5723 (0.9498)\tPrec@1 54.688 (75.867)\tPrec@5 85.938 (92.800)\n",
      "Test: [320/391]\tTime 0.478 (0.318)\tLoss 0.9346 (0.9483)\tPrec@1 73.438 (75.862)\tPrec@5 93.750 (92.840)\n",
      "Test: [330/391]\tTime 0.088 (0.317)\tLoss 0.8560 (0.9449)\tPrec@1 76.562 (75.946)\tPrec@5 96.094 (92.886)\n",
      "Test: [340/391]\tTime 0.852 (0.317)\tLoss 0.9360 (0.9434)\tPrec@1 79.688 (75.999)\tPrec@5 93.750 (92.916)\n",
      "Test: [350/391]\tTime 0.088 (0.317)\tLoss 0.6367 (0.9399)\tPrec@1 85.156 (76.053)\tPrec@5 96.875 (92.953)\n",
      "Test: [360/391]\tTime 0.716 (0.317)\tLoss 0.4917 (0.9344)\tPrec@1 89.844 (76.153)\tPrec@5 96.094 (93.034)\n",
      "Test: [370/391]\tTime 0.088 (0.316)\tLoss 1.0430 (0.9325)\tPrec@1 78.125 (76.205)\tPrec@5 92.188 (93.080)\n",
      "Test: [380/391]\tTime 0.459 (0.316)\tLoss 0.7217 (0.9330)\tPrec@1 78.906 (76.191)\tPrec@5 95.312 (93.075)\n",
      "Test: [390/391]\tTime 1.904 (0.319)\tLoss 1.2861 (0.9355)\tPrec@1 71.250 (76.144)\tPrec@5 86.250 (93.046)\n",
      "~~0\t0.31047149611111113\t93.046\n",
      "\n",
      " * Prec@1 76.144 Prec@5 93.046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.144"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "validate(val_loader, model, criterion, 0, start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AR with resize 1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/391]\tTime 2.462 (2.462)\tLoss 1.0381 (1.0381)\tPrec@1 73.438 (73.438)\tPrec@5 91.406 (91.406)\n",
      "Test: [10/391]\tTime 0.138 (0.494)\tLoss 0.9663 (0.8747)\tPrec@1 79.688 (77.202)\tPrec@5 92.188 (94.247)\n",
      "Test: [20/391]\tTime 0.123 (0.463)\tLoss 0.9185 (0.9419)\tPrec@1 78.125 (75.930)\tPrec@5 94.531 (93.824)\n",
      "Test: [30/391]\tTime 0.121 (0.408)\tLoss 0.7988 (0.9348)\tPrec@1 85.938 (76.260)\tPrec@5 92.969 (93.800)\n",
      "Test: [40/391]\tTime 0.133 (0.411)\tLoss 1.0264 (0.9115)\tPrec@1 73.438 (77.115)\tPrec@5 93.750 (94.074)\n",
      "Test: [50/391]\tTime 0.113 (0.387)\tLoss 1.1367 (0.9567)\tPrec@1 73.438 (76.149)\tPrec@5 91.406 (93.367)\n",
      "Test: [60/391]\tTime 0.113 (0.386)\tLoss 1.6260 (0.9970)\tPrec@1 56.250 (75.128)\tPrec@5 85.938 (92.841)\n",
      "Test: [70/391]\tTime 0.113 (0.373)\tLoss 1.0781 (0.9921)\tPrec@1 73.438 (75.253)\tPrec@5 92.969 (92.848)\n",
      "Test: [80/391]\tTime 0.105 (0.371)\tLoss 0.8721 (0.9677)\tPrec@1 76.562 (75.791)\tPrec@5 93.750 (93.142)\n",
      "Test: [90/391]\tTime 0.109 (0.361)\tLoss 0.8960 (0.9565)\tPrec@1 78.125 (75.953)\tPrec@5 96.094 (93.286)\n",
      "Test: [100/391]\tTime 0.088 (0.362)\tLoss 0.8213 (0.9394)\tPrec@1 78.125 (76.222)\tPrec@5 94.531 (93.487)\n",
      "Test: [110/391]\tTime 0.088 (0.353)\tLoss 1.1328 (0.9344)\tPrec@1 71.875 (76.351)\tPrec@5 89.844 (93.511)\n",
      "Test: [120/391]\tTime 0.088 (0.354)\tLoss 1.2432 (0.9561)\tPrec@1 71.875 (75.917)\tPrec@5 89.062 (93.188)\n",
      "Test: [130/391]\tTime 0.095 (0.348)\tLoss 0.8730 (0.9478)\tPrec@1 74.219 (75.984)\tPrec@5 92.188 (93.213)\n",
      "Test: [140/391]\tTime 0.100 (0.349)\tLoss 0.8003 (0.9357)\tPrec@1 79.688 (76.180)\tPrec@5 93.750 (93.351)\n",
      "Test: [150/391]\tTime 0.104 (0.345)\tLoss 0.5625 (0.9280)\tPrec@1 85.156 (76.371)\tPrec@5 97.656 (93.419)\n",
      "Test: [160/391]\tTime 0.109 (0.346)\tLoss 1.0293 (0.9153)\tPrec@1 74.219 (76.747)\tPrec@5 92.188 (93.527)\n",
      "Test: [170/391]\tTime 0.110 (0.342)\tLoss 0.6084 (0.9092)\tPrec@1 84.375 (76.919)\tPrec@5 96.875 (93.563)\n",
      "Test: [180/391]\tTime 0.112 (0.346)\tLoss 0.3875 (0.9023)\tPrec@1 90.625 (77.046)\tPrec@5 98.438 (93.590)\n",
      "Test: [190/391]\tTime 0.117 (0.343)\tLoss 0.6128 (0.9005)\tPrec@1 85.156 (77.049)\tPrec@5 96.094 (93.648)\n",
      "Test: [200/391]\tTime 0.112 (0.346)\tLoss 1.1836 (0.8907)\tPrec@1 65.625 (77.282)\tPrec@5 92.969 (93.754)\n",
      "Test: [210/391]\tTime 0.112 (0.343)\tLoss 0.8467 (0.8864)\tPrec@1 76.562 (77.377)\tPrec@5 96.094 (93.846)\n",
      "Test: [220/391]\tTime 0.112 (0.347)\tLoss 0.5015 (0.8814)\tPrec@1 88.281 (77.418)\tPrec@5 95.312 (93.941)\n",
      "Test: [230/391]\tTime 0.112 (0.344)\tLoss 0.5239 (0.8767)\tPrec@1 89.844 (77.547)\tPrec@5 97.656 (93.990)\n",
      "Test: [240/391]\tTime 0.112 (0.347)\tLoss 1.3008 (0.8858)\tPrec@1 63.281 (77.321)\tPrec@5 91.406 (93.915)\n",
      "Test: [250/391]\tTime 0.112 (0.345)\tLoss 0.9199 (0.8933)\tPrec@1 75.781 (77.151)\tPrec@5 92.969 (93.834)\n",
      "Test: [260/391]\tTime 0.112 (0.346)\tLoss 1.3320 (0.9000)\tPrec@1 66.406 (77.056)\tPrec@5 85.938 (93.717)\n",
      "Test: [270/391]\tTime 0.111 (0.345)\tLoss 1.0078 (0.9051)\tPrec@1 75.000 (76.932)\tPrec@5 90.625 (93.664)\n",
      "Test: [280/391]\tTime 0.112 (0.346)\tLoss 1.1553 (0.9104)\tPrec@1 76.562 (76.846)\tPrec@5 89.062 (93.558)\n",
      "Test: [290/391]\tTime 0.112 (0.343)\tLoss 1.3145 (0.9187)\tPrec@1 64.844 (76.646)\tPrec@5 89.844 (93.422)\n",
      "Test: [300/391]\tTime 0.112 (0.344)\tLoss 0.9146 (0.9228)\tPrec@1 71.875 (76.482)\tPrec@5 93.750 (93.366)\n",
      "Test: [310/391]\tTime 0.116 (0.342)\tLoss 1.4590 (0.9210)\tPrec@1 57.812 (76.532)\tPrec@5 86.719 (93.381)\n",
      "Test: [320/391]\tTime 0.118 (0.343)\tLoss 0.9023 (0.9198)\tPrec@1 77.344 (76.536)\tPrec@5 92.188 (93.404)\n",
      "Test: [330/391]\tTime 0.121 (0.342)\tLoss 0.7964 (0.9171)\tPrec@1 78.906 (76.619)\tPrec@5 96.094 (93.460)\n",
      "Test: [340/391]\tTime 0.128 (0.342)\tLoss 0.9033 (0.9155)\tPrec@1 80.469 (76.677)\tPrec@5 93.750 (93.484)\n",
      "Test: [350/391]\tTime 0.128 (0.341)\tLoss 0.6709 (0.9128)\tPrec@1 82.031 (76.763)\tPrec@5 96.094 (93.532)\n",
      "Test: [360/391]\tTime 0.129 (0.342)\tLoss 0.5132 (0.9091)\tPrec@1 89.844 (76.850)\tPrec@5 96.094 (93.596)\n",
      "Test: [370/391]\tTime 0.129 (0.341)\tLoss 1.0059 (0.9075)\tPrec@1 79.688 (76.908)\tPrec@5 92.969 (93.653)\n",
      "Test: [380/391]\tTime 0.129 (0.341)\tLoss 0.7344 (0.9076)\tPrec@1 79.688 (76.934)\tPrec@5 95.312 (93.647)\n",
      "Test: [390/391]\tTime 0.176 (0.342)\tLoss 0.9429 (0.9083)\tPrec@1 77.500 (76.894)\tPrec@5 92.500 (93.668)\n",
      "~~0\t0.3737668822222222\t93.668\n",
      "\n",
      " * Prec@1 76.894 Prec@5 93.668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.894"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset_ar_rs, batch_size=val_bs, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True, sampler=val_sampler_ar)\n",
    "\n",
    "validate(val_loader, model, criterion, 0, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
