{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, shutil, time, warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import models\n",
    "from fp16util import network_to_half, set_grad, copy_in_params\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n",
    "#print(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "    parser.add_argument('data', metavar='DIR', help='path to dataset')\n",
    "    parser.add_argument('--save-dir', type=str, default=Path.cwd(), help='Directory to save logs and models.')\n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet50')\n",
    "    # parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet18',\n",
    "    #                     choices=model_names,\n",
    "    #                     help='model architecture: ' +\n",
    "    #                     ' | '.join(model_names) +\n",
    "    #                     ' (default: resnet18)')\n",
    "    parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--epochs', default=45, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('--warmup', default=0, type=int, metavar='N',\n",
    "                        help='number of additional epochs to warmup')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 256)')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.4, type=float,\n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                        metavar='N', help='print frequency (default: 10)')\n",
    "    parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--small', action='store_true', help='start with smaller images')\n",
    "    parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                        help='evaluate model on validation set')\n",
    "    parser.add_argument('--pretrained', dest='pretrained', action='store_true', help='use pre-trained model')\n",
    "    parser.add_argument('--fp16', action='store_true', help='Run model fp16 mode.')\n",
    "    parser.add_argument('--dp', action='store_true', help='Run model fp16 mode.')\n",
    "    parser.add_argument('--sz',       default=224, type=int, help='Size of transformed image.')\n",
    "    parser.add_argument('--decay-int', default=30, type=int, help='Decay LR by 10 every decay-int epochs')\n",
    "    parser.add_argument('--loss-scale', type=float, default=1,\n",
    "                        help='Loss scaling, positive power of 2 values can improve fp16 convergence.')\n",
    "    parser.add_argument('--prof', dest='prof', action='store_true', help='Only run a few iters for profiling.')\n",
    "\n",
    "    parser.add_argument('--distributed', action='store_true', help='Run distributed training')\n",
    "    parser.add_argument('--dist-url', default='file://sync.file', type=str,\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='nccl', type=str, help='distributed backend')\n",
    "    parser.add_argument('--local_rank', default=0, type=int,\n",
    "                        help='Used for multi-process training. Can either be manually set ' +\n",
    "                        'or automatically set by using \\'python -m multiproc\\'.')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_path = str(Path.home()/'7-8x_train_lr3d2_e68_b128_93_success/model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "args = get_parser().parse_args(['/home/paperspace/data/imagenet', '--evaluate', '--resume', resume_path])\n",
    "if args.local_rank > 0: sys.stdout = open(f'{args.save_dir}/GPU_{args.local_rank}.log', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def sort_ar(valdir):\n",
    "    if os.path.isfile('sorted_idxar.p'): return pickle.load(open('sorted_idxar.p', 'rb'))\n",
    "    val_dataset = datasets.ImageFolder(valdir)\n",
    "    sizes = [img[0].size for img in val_dataset]\n",
    "    idx_ar = [(i, round(s[0]/s[1], 5)) for i,s in enumerate(sizes)]\n",
    "    sorted_idxar = sorted(idx_ar, key=lambda x: x[1])\n",
    "    pickle.dump(sorted_idxar, open('sorted_idxar.p', 'wb'))\n",
    "    return sorted_idxar\n",
    "\n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))\n",
    "\n",
    "def map_idx2ar(idx_ar_sorted, batch_size):\n",
    "    ar_chunks = list(chunks(idx_ar_sorted, batch_size))\n",
    "    idx2ar = {}\n",
    "    for chunk in ar_chunks:\n",
    "        idxs, ars = list(zip(*chunk))\n",
    "        mean = round(np.mean(ars), 5)\n",
    "        for idx in idxs:\n",
    "            idx2ar[idx] = mean\n",
    "    return idx2ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None, ar_transform=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "        self.ar_transform = ar_transform\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.ar_transform(sample, index)\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "class ARSampler(Sampler):\n",
    "    def __init__(self, indices): self.indices = indices\n",
    "    def __len__(self): return len(self.indices)\n",
    "    def __iter__(self): return iter(self.indices)\n",
    "    \n",
    "\n",
    "class CropArTfm(object):\n",
    "    def __init__(self, idx2ar, target_size):\n",
    "        self.idx2ar, self.target_size = idx2ar, target_size\n",
    "    def __call__(self, img, idx):\n",
    "        target_ar = self.idx2ar[idx]\n",
    "        if target_ar < 1: \n",
    "            w = int(self.target_size/target_ar)\n",
    "            size = (w//8*8, self.target_size)\n",
    "        else: \n",
    "            h = int(self.target_size*target_ar)\n",
    "            size = (self.target_size, h//8*8)\n",
    "        return torchvision.transforms.functional.center_crop(img, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation_set(valdir, batch_size, target_size, tensor_tfm, use_val_sampler, use_ar_sampler):\n",
    "    idx_ar_sorted = sort_ar(valdir)\n",
    "    idx_sorted, _ = zip(*idx_ar_sorted)\n",
    "    idx2ar = map_idx2ar(idx_ar_sorted, batch_size)\n",
    "    \n",
    "    if use_ar_sampler:\n",
    "        val_dataset = ValDataset(valdir, transforms.Compose(tensor_tfm), ar_transform=CropArTfm(idx2ar, target_size))\n",
    "        val_sampler = ARSampler(idx_sorted)\n",
    "        return val_dataset, val_sampler\n",
    "    \n",
    "    val_tfms = [transforms.Resize(int(args.sz*1.14)), transforms.CenterCrop(args.sz)] + tensor_tfm\n",
    "    val_dataset = datasets.ImageFolder(valdir,  transforms.Compose(val_tfms))\n",
    "    val_sampler = None\n",
    "    if use_val_sampler and args.distributed:\n",
    "        val_sampler = torch.utils.data.distributed.DistributedSampler(val_dataset)\n",
    "    return val_dataset, val_sampler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(traindir, valdir, bs, sz, val_bs=None, use_val_sampler=True, use_ar_sampler=False, min_scale=0.08):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    tensor_tfm = [transforms.ToTensor(), normalize]\n",
    "    \n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        traindir, transforms.Compose([\n",
    "            transforms.RandomResizedCrop(sz, scale=(min_scale, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "        ] + tensor_tfm))\n",
    "    train_sampler = (torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=bs, shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "    val_bs = val_bs or bs\n",
    "    val_dataset, val_sampler = create_validation_set(valdir, val_bs, sz, tensor_tfm, use_val_sampler, use_ar_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=val_bs, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True, sampler=val_sampler)\n",
    "\n",
    "    return train_loader,val_loader,train_sampler,val_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item() is a recent addition, so this helps with backward compatibility.\n",
    "def to_python_float(t):\n",
    "    if hasattr(t, 'item'):\n",
    "        return t.item()\n",
    "    else:\n",
    "        return t[0]\n",
    "\n",
    "class data_prefetcher():\n",
    "    def __init__(self, loader, prefetch=True):\n",
    "        self.loader,self.prefetch = iter(loader),prefetch\n",
    "        if prefetch:\n",
    "            self.stream = torch.cuda.Stream()\n",
    "            self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(async=True)\n",
    "            self.next_target = self.next_target.cuda(async=True)\n",
    "\n",
    "    def next(self):\n",
    "        if not self.prefetch:\n",
    "            input,target = next(self.loader)\n",
    "            return input.cuda(async=True),target.cuda(async=True)\n",
    "\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        input = self.next_input\n",
    "        target = self.next_target\n",
    "        self.preload()\n",
    "        return input, target\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch, start_time):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "    prefetcher = data_prefetcher(val_loader)\n",
    "    input, target = prefetcher.next()\n",
    "    i = -1\n",
    "    while input is not None:\n",
    "        i += 1\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = Variable(input)\n",
    "        target_var = Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "\n",
    "        if args.distributed:\n",
    "            reduced_loss = reduce_tensor(loss.data)\n",
    "            prec1 = reduce_tensor(prec1)\n",
    "            prec5 = reduce_tensor(prec5)\n",
    "        else:\n",
    "            reduced_loss = loss.data\n",
    "            \n",
    "\n",
    "        losses.update(to_python_float(reduced_loss), input.size(0))\n",
    "        top1.update(to_python_float(prec1), input.size(0))\n",
    "        top5.update(to_python_float(prec5), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if args.local_rank == 0 and i % args.print_freq == 0:\n",
    "            output = ('Test: [{0}/{1}]\\t' \\\n",
    "                    + 'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t' \\\n",
    "                    + 'Loss {loss.val:.4f} ({loss.avg:.4f})\\t' \\\n",
    "                    + 'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t' \\\n",
    "                    + 'Prec@5 {top5.val:.3f} ({top5.avg:.3f})').format(\n",
    "                    i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                    top1=top1, top5=top5)\n",
    "            print(output)\n",
    "            with open(f'{args.save_dir}/full.log', 'a') as f:\n",
    "                f.write(output + '\\n')\n",
    "\n",
    "        input, target = prefetcher.next()\n",
    "\n",
    "    time_diff = datetime.now()-start_time\n",
    "    print(f'~~{epoch}\\t{float(time_diff.total_seconds() / 3600.0)}\\t{top5.avg:.3f}\\n')\n",
    "    print(' * Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, f'{args.save_dir}/model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def reduce_tensor(tensor):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    size = dist.get_world_size()\n",
    "    # rt /= args.world_size\n",
    "    rt /= size\n",
    "    return rt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.fp16 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~epoch\thours\ttop1Accuracy\n",
      "\n",
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "print(\"~~epoch\\thours\\ttop1Accuracy\\n\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "if args.distributed:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url)\n",
    "    print('Distributed: init_process_group success')\n",
    "\n",
    "if args.fp16: assert torch.backends.cudnn.enabled, \"fp16 mode requires cudnn backend to be enabled.\"\n",
    "\n",
    "# create model\n",
    "#     if args.pretrained: model = models.__dict__[args.arch](pretrained=True)\n",
    "#     else: model = models.__dict__[args.arch]()\n",
    "# AS: force use resnet50 for now, until we figure out whether to upload model directory\n",
    "import resnet\n",
    "model = resnet.resnet50()\n",
    "\n",
    "print(\"Loaded model\")\n",
    "\n",
    "model = model.cuda()\n",
    "n_dev = torch.cuda.device_count()\n",
    "if args.fp16: model = network_to_half(model)\n",
    "if args.distributed: model = nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "elif args.dp:\n",
    "    model = nn.DataParallel(model)\n",
    "    args.batch_size *= n_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined loss and optimizer\n"
     ]
    }
   ],
   "source": [
    "global param_copy\n",
    "if args.fp16:\n",
    "    param_copy = [param.clone().type(torch.cuda.FloatTensor).detach() for param in model.parameters()]\n",
    "    for param in param_copy: param.requires_grad = True\n",
    "else: param_copy = list(model.parameters())\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(param_copy, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "print(\"Defined loss and optimizer\")\n",
    "\n",
    "best_prec1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DWrap(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    def forward(self, x):\n",
    "        return self.module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to wrap inside module - sinde checkpoint had a Distributed wrapper around it\n",
    "model = DWrap(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally resume from a checkpoint\n",
    "if args.resume:\n",
    "    if os.path.isfile(args.resume):\n",
    "        checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(0))\n",
    "#             checkpoint = torch.load(args.resume, map_location = lambda storage, loc: storage.cuda(args.gpu))\n",
    "        args.start_epoch = checkpoint['epoch']\n",
    "        best_prec1 = checkpoint['best_prec1']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    else: print(\"=> no checkpoint found at '{}'\".format(args.resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir = os.path.join(args.data, 'train')\n",
    "valdir = os.path.join(args.data, 'validation')\n",
    "train_loader,val_loader,train_sampler,val_sampler = get_loaders(\n",
    "    traindir, valdir, bs=128, val_bs=128, sz=288, use_val_sampler=False, use_ar_sampler=True, min_scale=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/391]\tTime 9.442 (9.442)\tLoss 1.5127 (1.5127)\tPrec@1 66.406 (66.406)\tPrec@5 87.500 (87.500)\n",
      "Test: [10/391]\tTime 0.504 (2.296)\tLoss 1.0293 (1.0240)\tPrec@1 79.688 (74.077)\tPrec@5 92.969 (92.543)\n",
      "Test: [20/391]\tTime 3.404 (1.579)\tLoss 1.0449 (1.0817)\tPrec@1 71.094 (72.693)\tPrec@5 92.969 (92.225)\n",
      "Test: [30/391]\tTime 0.443 (1.593)\tLoss 0.8716 (1.0731)\tPrec@1 80.469 (73.236)\tPrec@5 92.969 (91.935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-1b5ea672eff6>\", line 10, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"<ipython-input-9-1b5ea672eff6>\", line 7, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 76, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\", line 147, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 81, in to_tensor\n",
      "    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\", line 130, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/Image.py\", line 879, in convert\n",
      "    self.load()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-9-1b5ea672eff6>\", line 10, in __getitem__\n",
      "    sample = self.transform(sample)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\", line 76, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\", line 81, in to_tensor\n",
      "    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/ImageFile.py\", line 215, in load\n",
      "    s = read(self.decodermaxblock)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/JpegImagePlugin.py\", line 362, in load_read\n",
      "    s = self.fp.read(read_bytes)\n",
      "KeyboardInterrupt\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in default_collate\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 138, in <listcomp>\n",
      "    return [default_collate(samples) for samples in transposed]\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 115, in default_collate\n",
      "    return torch.stack(batch, 0, out=out)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 2612) exited unexpectedly with exit code 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2962\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2963\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2964\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-842a43294479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-f786a4eb92c6>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(val_loader, model, criterion, epoch, start_time)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_python_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mtop1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_python_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-f786a4eb92c6>\u001b[0m in \u001b[0;36mto_python_float\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 2612) exited unexpectedly with exit code 1."
     ]
    }
   ],
   "source": [
    "validate(val_loader, model, criterion, 0, start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
